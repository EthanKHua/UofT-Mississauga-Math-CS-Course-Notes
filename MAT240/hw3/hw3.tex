\documentclass{eh-homework}

\begin{document}
    \begin{question}{1}
        Let \( A = \begin{pmatrix} 1 & 0 & -1 & 1 \\ 1 & 1 & 1 & 0 \\ 2 & 1 & 0 & 1 \end{pmatrix} \). Use row and column operations on \( A \) to obtain a matrix \( B \) of the form in Theorem 53. Use that work to find invertible matrices \( P, Q \) so that \( B = PAQ \).
        \tcblower
        We perform the following row and column operations:
        \begin{align*}
            A =
            &\begin{pmatrix}
                1 & 0 & -1 & 1 \\
                1 & 1 & 1 & 0 \\
                2 & 1 & 0 & 1
            \end{pmatrix}
            \xrightarrow[r_3\,\to\,r_3 - 2r_1]{r_2\,\to\,r_2 - r_1}
            \begin{pmatrix}
                1 & 0 & -1 &  1 \\
                0 & 1 & 2 &  -1 \\
                0 & 1 & 2 &  -1 \\
            \end{pmatrix}
            \xrightarrow{r_3\,\to\,r_3 - r_2}
            \begin{pmatrix}
                1 & 0 & -1 &  1 \\
                0 & 1 & 2 &  -1 \\
                0 & 0 & 0 &  0 \\
            \end{pmatrix}\\
            \xrightarrow[c_4\,\to\,c_4 - c_1 + c_2]{c_3\,\to\,c_3 + c_1 - 2c_2}
            &\begin{pmatrix}
                1 & 0 & 0 &  0 \\
                0 & 1 & 0 &  0 \\
                0 & 0 & 0 &  0 \\
            \end{pmatrix}\\
        \end{align*}
        Define this matrix we obtained as \(B\). We will perform the same row and column operations above on \(I_3\) and \(I_4\), respectively in order to define \(P\) and \(Q\). We have that
        \begin{align*}
            \begin{pmatrix}
                1 & 0 &  0 \\
                0 & 1 &  0 \\
                0 & 0 &  1 \\
            \end{pmatrix}
            &\xrightarrow[r_3\,\to\,r_3 - 2r_1]{r_2\,\to\,r_2 - r_1}
            \begin{pmatrix}
                1 & 0 &  0 \\
                -1 & 1 &  0 \\
                -2 & 0 &  1 \\
            \end{pmatrix}\\
            &\xrightarrow{r_3\,\to\,r_3 - r_2}
            \begin{pmatrix}
                1 & 0 &  0 \\
                -1 & 1 &  0 \\
                -1 & -1 &  1 \\
            \end{pmatrix}
        \end{align*}
        and
        \begin{align*}
            \begin{pmatrix}
                1 & 0 & 0 &  0 \\
                0 & 1 & 0 &  0 \\
                0 & 0 & 1 &  0 \\
                0 & 0 & 0 &  1 \\
            \end{pmatrix}
            \xrightarrow[c_4\,\to\,c_4 - c_1 + c_2]{c_3\,\to\,c_3 + c_1 - 2c_2}
            \begin{pmatrix}
                1 & 0 & 1 &  -1 \\
                0 & 1 & -2 &  1 \\
                0 & 0 & 1 &  0 \\
                0 & 0 & 0 &  1 \\
            \end{pmatrix}
        \end{align*}
        Let \(P = \begin{pmatrix}
            1 & 0 &  0 \\
            -1 & 1 &  0 \\
            -1 & -1 &  1 \\
        \end{pmatrix},
        Q = \begin{pmatrix}
            1 & 0 & 1 &  -1 \\
            0 & 1 & -2 &  1 \\
            0 & 0 & 1 &  0 \\
            0 & 0 & 0 &  1 \\
        \end{pmatrix}\). We see that
        \begin{align*}
            PAQ &= \begin{pmatrix}
                1 & 0 &  0 \\
                -1 & 1 &  0 \\
                -1 & -1 &  1 \\
            \end{pmatrix}
            \begin{pmatrix}
                1 & 0 & -1 & 1 \\
                1 & 1 & 1 & 0 \\
                2 & 1 & 0 & 1
            \end{pmatrix}
            \begin{pmatrix}
                1 & 0 & 1 &  -1 \\
                0 & 1 & -2 &  1 \\
                0 & 0 & 1 &  0 \\
                0 & 0 & 0 &  1 \\
            \end{pmatrix} \\
            &=\begin{pmatrix}
                1 & 0 & -1 &  1 \\
                0 & 1 & 2 &  -1 \\
                0 & 0 & 0 &  0 \\
            \end{pmatrix}
            \begin{pmatrix}
                1 & 0 & 1 &  -1 \\
                0 & 1 & -2 &  1 \\
                0 & 0 & 1 &  0 \\
                0 & 0 & 0 &  1 \\
            \end{pmatrix}\\
            &=\begin{pmatrix}
                1 & 0 & 0 &  0 \\
                0 & 1 & 0 &  0 \\
                0 & 0 & 0 &  0 \\
            \end{pmatrix}\\
            &= B
        \end{align*}
        as required.
    \end{question}

    \begin{question}{2}
        Let \( A = \begin{pmatrix} 1 & -2 & -4 \\ 1 & 1 & -1 \\ 1 & 0 & -1 \end{pmatrix} \).

        \begin{enumerate}[label=(\alph*)]
            \item Verify that \( A \) is invertible, by row-reducing the augmented matrix \( (A | I_3) \).
            \item Use (a) to find \( A^{-1} \).
            \item Express \( A \) as a product of elementary matrices.
        \end{enumerate}
        \tcblower
        \ 
        
        (a):
        We see that 
        \begin{align*}
            (A|I_3) &= \left( \begin{array}{@{}ccc|ccc@{}}
                1 & -2 & -4 & 1 & 0 & 0 \\
                1 & 1 & -1 & 0 & 1 & 0 \\
                1 & 0 & -1 & 0 & 0 & 1 \\
            \end{array} \right)
            \xrightarrow{r_2\,\to\,r_2 - r_1,r_3\,\to\,r_3 - r_1}
            \left( \begin{array}{@{}ccc|ccc@{}}
                1 & -2 & -4 & 1 & 0 & 0 \\
                0 & 3 & 3 & -1 & 1 & 0 \\
                0 & 2 & 3 & -1 & 0 & 1 \\
            \end{array} \right) \\
            &\xrightarrow{r_1\,\to\,r_1 + r_3,r_2\,\to\,r_2 - r_3}
            \left( \begin{array}{@{}ccc|ccc@{}}
                1 & 0 & -1 & 0 & 0 & 1 \\
                0 & 1 & 0 & 0 & 1 & -1 \\
                0 & 2 & 3 & -1 & 0 & 1 \\
            \end{array} \right)
            \xrightarrow{r_3\,\to\,r_3 - 2r_2}
            \left( \begin{array}{@{}ccc|ccc@{}}
                1 & 0 & -1 & 0 & 0 & 1 \\
                0 & 1 & 0 & 0 & 1 & -1 \\
                0 & 0 & 3 & -1 & -2 & 3 \\
            \end{array} \right) \\
            &\xrightarrow{r_1\,\to\,r_1 + \frac{1}{3}r_3}
            \left( \begin{array}{@{}ccc|ccc@{}}
                1 & 0 & 0 & -\frac{1}{3} & -\frac{2}{3} & 2 \\
                0 & 1 & 0 & 0 & 1 & -1 \\
                0 & 0 & 3 & -1 & -2 & 3 \\
            \end{array} \right)
            \xrightarrow{r_3\,\to\,\frac{1}{3}r_3}
            \left( \begin{array}{@{}ccc|ccc@{}}
                1 & 0 & 0 & -\frac{1}{3} & -\frac{2}{3} & 2 \\
                0 & 1 & 0 & 0 & 1 & -1 \\
                0 & 0 & 1 & -\frac{1}{3} & -\frac{2}{3} & 1 \\
            \end{array} \right)
        \end{align*}
        Since \(A\) can be row reduced into the identity matrix, \(A\) is invertible.

        \medskip

        (b):

        By our row reductions above, we know that \(A^{-1} = \begin{pmatrix}
            -\frac{1}{3} & -\frac{2}{3} & 2 \\
            0 & 1 & -1 \\
            -\frac{1}{3} & -\frac{2}{3} & 1 \\
        \end{pmatrix}\).

        \medskip

        (c):

        To express \(A\) is a product of elementary matrices, we can apply the opposite row operations to the identity matrix in reverse order. That is,
        \[
            A = \begin{pmatrix}
                1 & 0 &  0 \\
                1 & 1 &  0 \\
                0 & 0 &  1 \\
            \end{pmatrix}
            \begin{pmatrix}
                1 & 0 &  0 \\
                0 & 1 &  0 \\
                1 & 0 &  1 \\
            \end{pmatrix}
            \begin{pmatrix}
                1 & 0 &  -1 \\
                0 & 1 &  0 \\
                0 & 0 &  1 \\
            \end{pmatrix}
            \begin{pmatrix}
                1 & 0 &  0 \\
                0 & 1 &  1 \\
                0 & 0 &  1 \\
            \end{pmatrix}
            \begin{pmatrix}
                1 & 0 &  0 \\
                0 & 1 &  0 \\
                0 & 2 &  1 \\
            \end{pmatrix}
            \begin{pmatrix}
                1 & 0 &  -\frac{1}{3} \\
                0 & 1 &  0 \\
                0 & 0 &  1 \\
            \end{pmatrix}
            \begin{pmatrix}
                1 & 0 &  0 \\
                0 & 1 &  0 \\
                0 & 0 &  3 \\
            \end{pmatrix}
        \]
    \end{question}

    \begin{question}{3}
        Find the explicit formula for the linear transformation \( T : \mathbb{Q}^4 \to \mathbb{Q}^3 \) which satisfies:

        \[
        T\begin{pmatrix} 1 \\ 0 \\ 0 \\ 0 \end{pmatrix} = \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}, \quad
        T\begin{pmatrix} 2 \\ 1 \\ 0 \\ 0 \end{pmatrix} = \begin{pmatrix} 0 \\ 1 \\ 1 \end{pmatrix}, \quad
        T\begin{pmatrix} 1 \\ 1 \\ 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}, \quad
        T\begin{pmatrix} 1 \\ 1 \\ 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}.
        \]
        \tcblower

        Notice that
        \[
            \beta = \left\{ \begin{pmatrix}
                 1 \\
                 0 \\
                 0 \\
                 0 \\
            \end{pmatrix},
            \begin{pmatrix}
                2 \\
                1 \\
                0 \\
                0 \\
            \end{pmatrix},
            \begin{pmatrix}
            1 \\
            1 \\
            1 \\
            0 \\
            \end{pmatrix},
            \begin{pmatrix}
                1 \\
                1 \\
                1 \\
                1 \\
            \end{pmatrix}
             \right\}
        \]
        is a basis for \(\mathbb{Q}^4\). We attempt to find the general form for a vector \((x,y,z,w) \in \mathbb{Q}^4\) in terms of these vectors. By inspection, we see that
        \[
            \begin{pmatrix}
                 x \\
                 y \\
                 z \\
                 w \\
            \end{pmatrix}
            =
            (x - 2y + z)
            \begin{pmatrix}
                1 \\
                0 \\
                0 \\
                0 \\
           \end{pmatrix}
            +(y - z)
            \begin{pmatrix}
                2 \\
                1 \\
                0 \\
                0 \\
           \end{pmatrix}
            +(z - w)
            \begin{pmatrix}
                1 \\
                1 \\
                1 \\
                0 \\
           \end{pmatrix}
           +
            w\begin{pmatrix}
                1 \\
                1 \\
                1 \\
                1 \\
           \end{pmatrix}
        \]
        Thus
        \begin{align*}
            T \begin{pmatrix}
                 x \\
                 y \\
                 z \\
                 w \\
            \end{pmatrix}
            &=
            (x - 2y + z)
            T\begin{pmatrix}
                1 \\
                0 \\
                0 \\
                0 \\
            \end{pmatrix}
            +(y - z)
            T\begin{pmatrix}
                2 \\
                1 \\
                0 \\
                0 \\
            \end{pmatrix}
            +(z - w)
            T\begin{pmatrix}
                1 \\
                1 \\
                1 \\
                0 \\
            \end{pmatrix}
            +wT\begin{pmatrix}
                1 \\
                1 \\
                1 \\
                1 \\
           \end{pmatrix} \\
           &=
           (x - 2y + z)
           \begin{pmatrix}
             1 \\
             2 \\
             3 \\
           \end{pmatrix}
           +(y-z)
           \begin{pmatrix}
             0 \\
             1 \\
             1 \\
           \end{pmatrix}
           +(z-w)
           \begin{pmatrix}
             0 \\
             0 \\
             1 \\
           \end{pmatrix}
           +w
           \begin{pmatrix}
             0 \\
             0 \\
             0 \\
           \end{pmatrix} \\
           &=\begin{pmatrix}
             x - 2y + z \\
             2x -3y + z \\
             3x - 5y + 3z - w \\
           \end{pmatrix}
        \end{align*}
    \end{question}

    \begin{question}{4}
        Let \( \mathbb{F} = \mathbb{Q} \) and \( V = \mathcal{M}_{2 \times 2}(\mathbb{F}) \). Consider the linear map \( T : \mathcal{M}_{2 \times 2}(\mathbb{F}) \to \mathcal{M}_{2 \times 2}(\mathbb{F}) \) given by \( T(A) = A^T \). Set \( \beta = \{ E_{11}, E_{12}, E_{21}, E_{22} \} \) and \( \gamma = \{ E_{11}, E_{22}, E_{12} + E_{21}, E_{12} - E_{21} \} \).

        \begin{enumerate}[label=(\alph*)]
            \item Find \( P \) - the change of coordinate matrix from \( \gamma \) to \( \beta \) coordinates.
            
            We have
            \begin{align*}
                P &= \begin{pmatrix}
                    [E_{11}]_\beta & [E_{22}]_\beta & [E_{12} + E_{21}]_\beta &  [E_{12} - E_{21}]_\beta \\
                \end{pmatrix} \\
                &= \begin{pmatrix}
                    1 & 0 & 0 & 0  \\
                    0 & 0 & 1 & 1  \\
                    0 & 0 & 1 & -1  \\
                    0 & 1 & 0 &  0 \\
                \end{pmatrix}
            \end{align*}
            \item Find \( P^{-1} \) - the change of coordinate matrix from \( \beta \) to \( \gamma \) coordinates.
            
            Similarly,
            \begin{align*}
                P^{-1} &= \begin{pmatrix}
                    [E_{11}]_\gamma & [E_{12}]_\gamma & [E_{21}]_\gamma & [E_{22}]_\gamma \\
                \end{pmatrix} \\
                &= \begin{pmatrix}
                    1 & 0 & 0 & 0  \\
                    0 & 0 & 0 & 1  \\
                    0 & \frac{1}{2} & \frac{1}{2} & 0  \\
                    0 & \frac{1}{2} & -\frac{1}{2} & 0  \\
                \end{pmatrix}
            \end{align*}
            \item Find \( A = [T]_\beta \).
            
            We see that
            \begin{align*}
                A &= \begin{pmatrix}
                    [T(E_{11})]_\beta & [T(E_{12})]_\beta & [T(E_{21})]_\beta & [T(E_{22})]_\beta \\
                \end{pmatrix} \\
                &= \begin{pmatrix}
                    1 & 0 & 0 & 0  \\
                    0 & 0 & 1 & 0  \\
                    0 & 1 & 0 & 0  \\
                    0 & 0 & 0 & 1  \\
                \end{pmatrix}
            \end{align*}
            \item Find \( B = [T]_\gamma \).
            
            Once again,
            \begin{align*}
                B &= \begin{pmatrix}
                    [T(E_{11})]_\gamma & [T(E_{22})]_\gamma & [T(E_{12} + E_{21})]_\gamma &  [T(E_{12} - E_{21})]_\gamma \\
                \end{pmatrix} \\
                &= \begin{pmatrix}
                    1 & 0 & 0 & 0  \\
                    0 & 1 & 0 & 0  \\
                    0 & 0 & 1 & 0  \\
                    0 & 0 & 0 & -1 \\
                \end{pmatrix}
            \end{align*}

            \item Confirm that \( A = PBP^{-1} \) using (a)-(d).
            
            \begin{align*}
                PBP^{-1} &=
                \begin{pmatrix}
                    1 & 0 & 0 & 0  \\
                    0 & 0 & 1 & 1  \\
                    0 & 0 & 1 & -1  \\
                    0 & 1 & 0 &  0 \\
                \end{pmatrix}
                \begin{pmatrix}
                    1 & 0 & 0 & 0  \\
                    0 & 1 & 0 & 0  \\
                    0 & 0 & 1 & 0  \\
                    0 & 0 & 0 & -1 \\
                \end{pmatrix}
                \begin{pmatrix}
                    1 & 0 & 0 & 0  \\
                    0 & 0 & 0 & 1  \\
                    0 & \frac{1}{2} & \frac{1}{2} & 0  \\
                    0 & \frac{1}{2} & -\frac{1}{2} & 0  \\
                \end{pmatrix} \\
                &= \begin{pmatrix}
                    1 & 0 & 0 &  0 \\
                    0 & 0 & 1 &  -1 \\
                    0 & 0 & 1 &  1 \\
                    0 & 1 & 0 &  0 \\
                \end{pmatrix}
                \begin{pmatrix}
                    1 & 0 & 0 & 0  \\
                    0 & 0 & 0 & 1  \\
                    0 & \frac{1}{2} & \frac{1}{2} & 0  \\
                    0 & \frac{1}{2} & -\frac{1}{2} & 0  \\
                \end{pmatrix} \\
                &=
                \begin{pmatrix}
                    1 & 0 & 0 &  0 \\
                    0 & 0 & 1 &  0 \\
                    0 & 1 & 0 &  0 \\
                    0 & 0 & 0 &  1 \\
                \end{pmatrix} \\
                &= A
            \end{align*}
            as expected.
        \end{enumerate}
    \end{question}

    \begin{question}{5}
        Let \( T : \mathcal{M}_{n \times n}(\mathbb{F}) \to \mathcal{M}_{n \times n}(\mathbb{F}) \) be the linear map given by \( T(A) = A + A^T \).

        \begin{enumerate}[label=(\alph*)]
            \item Find \( N(T) \) and \( \dim N(T) \).
            
            We claim that \(N(T)\) is the set of all skew symmetric matrices with zeroes on the diagonal, which has dimension \(\frac{1}{2}n(n-1)\).
            
            Set \(T(A) = A + A^T = 0\). We have that \(A_{ij} + A_{ji} = 0\) for each \(0 < i,j \leq n\). In particular, we have that \(A_{ij} = 0\) if \(i = j\) and \(A_{ij} = -A_{ji}\) otherwise. But this describes exactly all skew symmetric matrices with zeroes on the diagonal. The basis for this set is
            \[
                \beta = \{ E_{ij} - E_{ji} : 0 < i < j \leq n \}
            \]
            and there are \(\frac{1}{2}n(n-1)\) vectors in this set, so \(\dim N(T) = \frac{1}{2}n(n-1)\).
            \item What is \( \operatorname{im}(T) \)?
            
            We claim that \(\mathrm{im} (T)\) is the set of all symmetric matrices \(S_n\). We see that
            \[
                (A + A^t)_{ij} = A_{ij} + A^t_{ij} = A_{ij} + A_{ji} = A_{ji} + A^t_{ji} = (A + A^t)_{ji}
            \]
            so \(\mathrm{im} (T) \subseteq S_n\). To show set equality, suppose that \(B\) is a symmetric matrix. Let \(A = \frac{1}{2}B\) then
            \[
                T(A) = \frac{1}{2}T(B) = \frac{1}{2}(B + B^t) = B
            \]
            Thus \(\mathrm{im} (T) = S_n\) and has basis
            \[
                \gamma = \{ E_{ij} : 0 < i \leq j \leq n \}.
            \]
            and is dimension \(\frac{1}{2}n(n+1)\).
            \item Is \( \mathcal{M}_{n \times n}(\mathbb{F}) = \operatorname{im}(T) \oplus N(T) \)?
            
            Yes.

            To show this, notice that \(\beta \cap \gamma = \varnothing\), so \(\mathrm{im} (T) \oplus N(T)\) has basis \(\alpha = \beta \cup \gamma\). But notice that \(|\alpha| = \frac{1}{2}n(n-1) + \frac{1}{2}n(n+1) = n^2\), which is the dimension of \(\mathcal{M} _n(\mathbb{F})\). Therefore \(\alpha\) is actually a basis for \(\mathcal{M} _n(\mathbb{F})\) and thus \(\mathcal{M} _n(\mathbb{F}) = \mathrm{im} (T) \oplus N(T)\).
        \end{enumerate}
    \end{question}

    \begin{question}{6}
        Let \( V, W \) be vector spaces over a field \( \mathbb{F} \) and \( T : V \to W \) a linear map. Prove that \( T \) is injective if and only if \( N(T) = \{\mathbf{0}_V\} \). (Make no assumption here about \( \dim V, \dim W \).)
        \tcblower
        Suppose that \(T\) is injective. Let \(T(x) = 0\), for some \(x \in V\). Recall that \(T(0) = 0\) for any linear map. Therefore by injectivity \(x = 0\), so \(N(T) = \{ 0 \}\).

        Conversely, suppose that \(N(T) = \{ 0 \}\). Let \(x,y \in V\) such that \(T(x) = T(y)\). By linearity, we have that \(T(x - y) = 0\), but this implies that \(x - y = 0\), so \(x = y\) and \(T\) is injective.
    \end{question}

    \begin{question}{7}
        Let \( V, W \) be vector spaces over a field \( \mathbb{F} \), and \( T : V \to W \) a linear map. Find a condition on \( T \) which is equivalent to "\( T(S) \) spans \( W \) for any spanning set \( S \subseteq V \) of \( V \)".

        (Hint: Write down the definition of \( T(S) \) is spanning to get started.)
        \tcblower
        We claim that this statement is equivalent to saying that \(T\) is surjective.

        Suppose that for any set \(S \subseteq V\) that spans \(V\), \(T(S)\) spans \(W\). We prove that \(T\) is surjective.

        Let \(w \in W\). We can write \(w\) as a linear combination of some number of vectors in \(T(S)\). That is, for some \(k \in \mathbb{N}\) and \(s_i \in S\), \(c_i \in \mathbb{F}\), \(i \in \{ 1, ..., k \}\),
        \[
            w = \sum_{i=1}^{k} c_i T(s_i) = T \left(\sum_{i=1}^{k}c_i s_i\right)
        \]
        so \(T\) is surjective.

        Conversely, suppose that \(T\) is surjective. Let \(S\) be a spanning set of \(V\). We will show that \(T(S)\) spans \(W\). Let \(w \in W\). By surjectivity, there exists \(v \in V\) so that \(T(v) = w\). We can rewrite
        \[
            v = \sum_{i=1}^{k} c_i s_i
        \]
        for some number of vectors \(s_i \in S\) and \(c_i \in \mathbb{F}\). Then
        \[
            T\left( \sum_{i=1}^{k} c_i s_i \right) = w \implies \sum_{i=1}^{k} c_i T(s_i) = w
        \]
        Notice that \(T(s_i) \in T(S)\), from which it follows that \(T(S)\) spans \(W\), and the proof is complete.
    \end{question}

    \begin{question}{8}
        Let \( P \in \mathcal{M}_{n \times n}(\mathbb{F}) \). Prove the following three conditions are equivalent.

        \begin{enumerate}[label=(\alph*)]
            \item \( P \) is invertible.
            \item There exists bases \( \beta, \gamma \) of \( \mathbb{F}^n \) so that \( P = [\mathrm{I}_{\mathbb{F}^n}]_\beta^\gamma \).
            \item For any \( n \)-dimensional vector space \( V \) over \( \mathbb{F} \), there exists bases \( \beta, \gamma \) of \( V \) so that \( P = [\mathrm{I}_V]_\beta^\gamma \).
        \end{enumerate}
        \tcblower

        Suppose (a). We prove (b) and (c) at the same time.

        Let \(\beta = \{ v_1, ..., v_n \}, \beta' = \{ v'_1, ..., v'_n \}\) be bases for \(\mathbb{F}^n\) and \(V\) respectively. For \(i = 1, ..., n\), let
        \[
            u_i = \sum_{j=1}^{n} P_{ji} v_j \text{ and } u'_i = \sum_{j=1}^{n} P_{ji} v'_j.
        \]
        Define \(\gamma = \{ u_1, ...,u_n \}, \gamma ' = \{ u'_1, ..., u'_n \}\). We claim that they are bases for \(\mathbb{F}^n\) and \(V\). We will only show that is the case for \(\gamma\), because the argument is the same for \(\gamma '\).

        It suffices to show that \(\gamma\) is linearly independent, as it is a set of \(n\) vectors, from which it will follow that \(\gamma\) is a basis for \(\mathbb{F}^n\). For constants \(c_i \in \mathbb{F}\), let
        \[
            \sum_{i=1}^{n} c_i \sum_{j=1}^{n} P_{ji} v_j = 0
        \]
        By the linear independence of \(\beta\), for each \(j\),
        \[
            \sum_{i=1}^{n} c_i P_{ji} = 0
        \]
        But this is the same as saying
        \[
            P \begin{pmatrix}
                 c_1 \\
                 \vdots \\
                 c_n \\
            \end{pmatrix}
            = \vec{0}
        \]
        Since \(P^{-1}\) exists, we perform left multiplication by \(P^{-1}\) to see that
        \[
            \begin{pmatrix}
                c_1 \\
                \vdots \\
                c_n \\
           \end{pmatrix} = 0
        \]
        which means that \(c_i = 0\) for all \(i\), thus showing that \(\gamma\) is linearly independent and indeed a basis for \(\mathbb{F}^n\), and by the same argument, \(\gamma'\) is also a basis for \(V\).

        Finally, notice that for each \(u_i\), \([u_i]_\beta\) is equal to the \(i\)th row of \(P\) which confirms that \(P = [I_{\mathbb{F}^n}]_\gamma^\beta\). The same applies for \(u'_i\) so \(P = [I_V]_{\gamma'}^{\beta'}\).

        Consider the linear transformation \(T_P : \mathbb{F}^n \to \mathbb{F}^n\). Let \(\beta\) be an ordered basis for \(\mathbb{F}^n\). We will show that \(\gamma = T_P(\beta)\) is also an ordered basis for \(\mathbb{F}^n\). Since \(P\) is invertible, \(T_P\) has an inverse \((T_P)^{-1} = T_{P^{-1}}\), so \(T_P\) is surjective and \(\mathrm{span} (T_P(\beta)) = \mathbb{F}^n\). Since \(|T_P(\beta)| = n\), \(T_P(\beta)\) is indeed an ordered basis. Thus we can conclude that \(P\) is a change of basis matrix from \(\beta\) to \(\gamma\).

        \medskip

        Suppose (c). We prove (a).

        For some bases \(\beta , \gamma\), \(P = [I_V]_\beta^\gamma\). We claim that \(P^{-1} = [I_V]_\gamma^{\beta}\). Indeed,
        \[
            P P^{-1} =  [I_V]_\beta^\gamma [I_V]_\gamma^{\beta} = [I_V]_\gamma = I_n
        \]
        This covers all the equivalences and we are done.
    \end{question}

    \begin{question}{9}
        Consider the relation \( \equiv \) on \( \mathcal{M}_{m \times n}(\mathbb{F}) \) defined by \( A \equiv B \) if \( A \to B \) using a combination of row and/or column operations.

        \begin{enumerate}[label=(\alph*)]
            \item Prove that \( \equiv \) is an equivalence relation on \( \mathcal{M}_{m \times n}(\mathbb{F}) \).
            \item Find a condition on \( A, B \) which is equivalent to \( A \equiv B \). (Hint: Theorem 53.)
            \item Classify the equivalence classes for this relation, and prove that there are exactly \( 1 + \min\{n, m\} \) such classes.
        \end{enumerate}
        \tcblower
        \ 

        (a):

        We show reflexivity, symmetry, and transitivity in that order.

        Reflexivity: Since \(IA = A\), and \(I\) is considered a row operation, \(A \equiv A\).

        Symmetry: Suppose that \(A \equiv B\) then for some invertible matrices \(P,Q\) we have that \(PAQ = B\). But at the same time this means that \(P^{-1} B Q^{-1} = A\) so \(B \equiv A\).

        Transitivity: Suppose that \(A \equiv B\) and \(B \equiv C\). Then for invertible matrices \(P,Q,R,S\), \(PAQ = B\) and \(RBS = C\), so \((RP)A(QS) = R(PAQ)S = RBS = C\). Since \(RP, QS\) are also invertible, we have that \(A \equiv C\).

        \medskip

        (b):

        We claim that an equivalent condition is \(\rank A = \rank B\). Suppose that \(A \equiv B\). Then \(PAQ = B\) for some invertible matrices \(P,Q\), but it is known that rank is preserved by multiplication with invertible matrices, so \(\rank A = \rank PAQ = \rank B\).

        Conversely, suppose that \(r \coloneqq \rank A = \rank B\). By Theorem 53, there exist row/column operations so that
        \[
            A,B \to \left( \begin{array}{@{}c|c@{}}
                I_r & 0 \\
                \hline
                0 & 0
            \end{array} \right).
        \]
        We denote this matrix by \(J_r\).
        that is, for invertible matrices \(P,Q,R,S\), \(PAQ = I' = RBS\). It follows that \(R^{-1} P A Q S^{-1} = B\), so \(A \equiv B\) as desired.

        \medskip

        (c):

        We can classify the equivalence classes by matrix rank. That is, each equivalence class is of the form
        \[
            [J_r] = \{ A \in \mathcal{M} _{m\times n}(\mathbb{F}) : \rank A = r\}.
        \]
        The possible ranks of \(m\times n\) matrices range from \(0\) to \(\min \{ n,m \}\), so there are \(\min \{ n,m \} + 1\) different values of \(r\). We will verify that these equivalence classes are exhaustive and disjoint. Every \(m \times n\) matrix must have a rank, so it belongs to at least one of the classes, but at the same time, a matrix can possibly only have one rank, so it necessarily belongs to exactly one equivalence class.
    \end{question}

    \begin{question}{10}
        Let \( V, W \) be finite dimensional vector spaces over \( \mathbb{F} \), and \( T : V \to W \) a linear map with \(\rank T = 2\). Set \( n = \dim V \), \( m = \dim W \). Let \( \mathbf{x}_1, \mathbf{x}_2 \in \mathbb{F}^n \) be two non-parallel vectors. Prove there exists bases \( \beta, \gamma \) of \( V, W \) respectively, so that \( [T]_\beta^\gamma = (\mathbf{x}_1 \ \mathbf{x}_2 \ \mathbf{0} \ \cdots \ \mathbf{0}) \). (Hint: use problems 7,8.)

        \tcblower

        By the Dimension Theorem, \(\mathrm{null}(T) = n-2 \). Let \(\alpha = \{ v_1, ..., v_{n-2} \}\) be a basis for \(N(T)\) and extend this basis into a basis \(\{ a, b, v_1, ..., v_{n-2}\}\) which we set as \(\gamma\).
    \end{question}

    \begin{question}{11}
        Let \( T : V \to V \) be linear. We say that a subspace \( W \subseteq V \) is ``\( T \)-invariant'' if \( T(W) \subseteq W \).

        For example, if \( T : \mathbb{R}^3 \to \mathbb{R}^3 \) is counter-clockwise rotation around the \( z \)-axis by angle \( \theta \), then \( P_{xy} = \{(x, y, 0) \in \mathbb{R}^3\} \) is \( T \)-invariant, as is \( L_z \) (the \( z \)-axis).

        \begin{enumerate}[label=(\alph*)]
            \item Verify the claims made above, by showing that \( P_{xy} \) and \( L_z \) are \( T \)-invariant.
            \item Show that \( \mathbb{R}^3 = P_{xy} \oplus L_z \) by finding a basis \( \beta = \beta_1 \cup \beta_2 \) for \( \mathbb{R}^3 \) so that \( \beta_1 \) is a basis for \( P_{xy} \) and \( \beta_2 \) is a basis for \( L_z \).
            \item Using your basis \( \beta \) from (b), find \( [T]_\beta \).
        \end{enumerate}
        \tcblower
        \ 

        (a):

        We begin by finding an expression for \(T\). Notice that
        \begin{align*}
            T(e_1) &= (\cos \theta , \sin \theta , 0) \\
            T(e_2) &= (-\sin \theta, \cos \theta, 0) \\
            T(e_3) &= (0,0,1) \\
        \end{align*}
        In the case of \(e_1, e_2\), the projection onto the \(xy\)-plan lies on the unit circle, and thus each vector is rotated \(\theta\) and \(\theta + \frac{\pi}{2}\) radians respectively (relative to the point \((0,1)\)). Thus we have that
        \begin{align*}
            T(x,y,z) &= x(\cos \theta , \sin \theta , 0) + y(-\sin \theta , \cos \theta , 0) + z(0,0,1) \\
            &= (x\cos \theta - y\sin \theta , x\sin \theta +y\cos \theta , z)
        \end{align*}
        Now, let \((x,y,0) \in P_{xy}\). Then
        \[
            T(x,y,0) = (x \cos \theta - y\sin \theta , x\sin \theta + y\cos \theta , 0) \in P_{xy}
        \]
        Additionally, let \((0,0,z) \in L_z\). Then
        \[
            T(0,0,z) = (0,0,z) \in L_z
        \]
        Thus \(P_{xy}\) and \(L_z\) are T-invariant subspaces.

        \medskip

        (b):

        Let \(\beta_1 = \{ e_1, e_2 \}, \beta _2 = \{ e_3 \}\). It is clear that \(\beta _1\) is a basis for the \(xy\)-plane and \(\beta _2\) is a basis for the \(z\)-axis. Then \(\beta = \{ e_1, e_2, e_3 \}\) is the standard ordered basis for \(\mathbb{R}^3\), which was what we wanted to show.

        \medskip

        (c):

        We have already found all we need from the previous parts:
        \[
            [T]_\beta = \begin{pmatrix}
                [T(e_1)]_\beta & [T(e_2)]_\beta &  [T(e_3)]_\beta \\
            \end{pmatrix}
            =
            \begin{pmatrix}
                \cos \theta & \sin \theta &  0 \\
                -\sin \theta & \cos \theta &  0 \\
                0 & 0 &  1 \\
            \end{pmatrix}
        \]
    \end{question}

    \begin{question}{12}
        Let \( V \) be a finite dimensional vector space over \( \mathbb{F} \), \( T \in \mathcal{L}(V) \), and \( W_1 \subseteq V \) a \( T \)-invariant subspace with basis \( \beta_1 \). Set \( k = \dim W_1 \).

        We will generalize what we saw in \#11c.

        \begin{enumerate}[label=(\alph*)]
            \item Extend \( \beta_1 \) to a basis \( \beta \) of \( V \). Show that \( [T]_\beta = \begin{pmatrix} A & C \\ O_{n-k,k} & B \end{pmatrix} \), where \( A \) is \( k \times k \), \( B \) is \( (n-k) \times (n-k) \), and \( C \) is \( k \times (n-k) \).
            \item Suppose that \( W_2 \) is a subspace so that \( V = W_1 \oplus W_2 \). Let \( \beta = \beta_1 \cup \beta_2 \), where \( \beta_2 \) is any basis for \( W_2 \).

            Prove that if \( W_2 \) is \( T \)-invariant, then \( [T]_\beta = \begin{pmatrix} A & O_{k,n-k} \\ O_{n-k,k} & B \end{pmatrix} \) is block diagonal.
            \item Is the converse of (b) true or false? Justify your answer.
        \end{enumerate}
    \end{question}

    \begin{question}{13}
        Determine if the statements below are true or false. If true, give a proof. If false, explain why, and/or provide a counterexample.

        \begin{enumerate}[label=(\alph*)]
            \item Let \( \beta = \{ e_1, \ldots, e_n \} \) be the standard basis for \( \mathbb{F}^n \), and \( \gamma = \{ v_1, \ldots, v_n \} \) a basis for \( \mathbb{F}^n \). Then there exists a sequence of row operations that takes \( \beta \) to \( \gamma \). (That is, \( v_i \) is obtained from \( e_i \) using the same row operations for all \( i \).)
            \item Let \( V \) be a finite dimensional vector space over \( \mathbb{F} \) and \( T : V \to V \) a linear map. If \( \beta, \gamma \) are bases for \( V \) so that \( [T]_\beta^\gamma = I_n \), then \( T = I_V \).
            \item Let \( V \) be a finite dimensional vector space over \( \mathbb{F} \) and \( S, T : V \to V \) linear maps. If rank \( T = \text{rank } S \), then there exist bases \( \beta, \beta', \gamma, \gamma' \) for \( V \) so that \( [S]_\beta^\gamma = [T]_{\beta'}^{\gamma'}\).
            \item Let \( A, B \in \mathcal{M}_{n \times n}(\mathbb{F}) \). If \( A^2 \sim B^2 \), then \( A \sim B \).
        \end{enumerate}
        \tcblower
        \ 

        (a):

        This statement is true. Consider the linear operator \(T: \mathbb{F}^n \to \mathbb{F}^n\) defined by \(T(e_i) = v_i\) for all \(i \in \{ 1, ..., n \}\). Notice that \(T\) is surjective, as \(\mathrm{span} (T(\beta)) = \mathrm{span} (\gamma) = \mathbb{F}^n\). It follows that \(T\) is invertible, so \([T]_\beta^\gamma\) is invertible, so it can be decomposed into a number of elementary matrices and thus represent a sequence of row operations. But notice that \([T]_\beta^\gamma\) is exactly the matrix that maps \(e_i\) to \(v_i\), so we have what we wanted.

        \medskip

        (b):

        This is false. Let \(V = \mathbb{F}^n\). Take \(\beta\) as the standard basis. As well, let \(\gamma = \{ e_1, e_2, ..., e_{n-1}, -e_n\}\), so that \(\gamma\) is just \(\beta\) only with the last basis vector multiplied by \(-1\). Consider the linear map \(T\) defined in the previous part. Then \(T(e_n) = -e_n\), so \(T\) is not the identity map, but it is not hard to see that \([T]_\beta^\gamma = I_n\).

        \medskip

        (c):

        This is true. Let \(k = \rank \ T = \rank \ S\). Take \(k\) vectors \(e_1, ..., e_k \). By Question 10, there exists bases \(\beta , \gamma , \beta ', \gamma '\) such that
        \[
            [T]_\beta^\gamma = \left(\begin{array}{c|c}
                I_k & O \\
                O & O \\
            \end{array} \right)
            =
            [S]_{\beta'}^{\gamma'}
        \]
        as desired.

        \medskip

        (d):

        This is false. Consider \(A = \begin{pmatrix}
            0 &  1 \\
            0 &  0 \\
        \end{pmatrix},
        B = 0_n\), the zero matrix. Then
        \[
            A^2 = B^2
        \]
        But \(\rank A = 1\) and \(\rank B = 0\), so for any invertible matrix \(P\),
        \[
            \rank PAP^{-1} = 1
        \]
        so \(A\) is not similar to \(B\).
    \end{question}
\end{document}