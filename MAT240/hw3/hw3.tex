\documentclass{eh-homework}

\begin{document}
    \begin{question}{1}
        Let \( A = \begin{pmatrix} 1 & 0 & -1 & 1 \\ 1 & 1 & 1 & 0 \\ 2 & 1 & 0 & 1 \end{pmatrix} \). Use row and column operations on \( A \) to obtain a matrix \( B \) of the form in Theorem 53. Use that work to find invertible matrices \( P, Q \) so that \( B = PAQ \).
        \tcblower
        We perform the following row and column operations:
        \begin{align*}
            A =
            &\begin{pmatrix}
                1 & 0 & -1 & 1 \\
                1 & 1 & 1 & 0 \\
                2 & 1 & 0 & 1
            \end{pmatrix}
            \xrightarrow[r_3\,\to\,r_3 - 2r_1]{r_2\,\to\,r_2 - r_1}
            \begin{pmatrix}
                1 & 0 & -1 &  1 \\
                0 & 1 & 2 &  -1 \\
                0 & 1 & 2 &  -1 \\
            \end{pmatrix}
            \xrightarrow{r_3\,\to\,r_3 - r_2}
            \begin{pmatrix}
                1 & 0 & -1 &  1 \\
                0 & 1 & 2 &  -1 \\
                0 & 0 & 0 &  0 \\
            \end{pmatrix}\\
            \xrightarrow[c_4\,\to\,c_4 - c_1 + c_2]{c_3\,\to\,c_3 + c_1 - 2c_2}
            &\begin{pmatrix}
                1 & 0 & 0 &  0 \\
                0 & 1 & 0 &  0 \\
                0 & 0 & 0 &  0 \\
            \end{pmatrix}\\
        \end{align*}
        Define this matrix we obtained as \(B\). We will perform the same row and column operations above on \(I_3\) and \(I_4\), respectively in order to define \(P\) and \(Q\). We have that
        \begin{align*}
            \begin{pmatrix}
                1 & 0 &  0 \\
                0 & 1 &  0 \\
                0 & 0 &  1 \\
            \end{pmatrix}
            &\xrightarrow[r_3\,\to\,r_3 - 2r_1]{r_2\,\to\,r_2 - r_1}
            \begin{pmatrix}
                1 & 0 &  0 \\
                -1 & 1 &  0 \\
                -2 & 0 &  1 \\
            \end{pmatrix}\\
            &\xrightarrow{r_3\,\to\,r_3 - r_2}
            \begin{pmatrix}
                1 & 0 &  0 \\
                -1 & 1 &  0 \\
                -1 & -1 &  1 \\
            \end{pmatrix}
        \end{align*}
        and
        \begin{align*}
            \begin{pmatrix}
                1 & 0 & 0 &  0 \\
                0 & 1 & 0 &  0 \\
                0 & 0 & 1 &  0 \\
                0 & 0 & 0 &  1 \\
            \end{pmatrix}
            \xrightarrow[c_4\,\to\,c_4 - c_1 + c_2]{c_3\,\to\,c_3 + c_1 - 2c_2}
            \begin{pmatrix}
                1 & 0 & 1 &  -1 \\
                0 & 1 & -2 &  1 \\
                0 & 0 & 1 &  0 \\
                0 & 0 & 0 &  1 \\
            \end{pmatrix}
        \end{align*}
        Let \(P = \begin{pmatrix}
            1 & 0 &  0 \\
            -1 & 1 &  0 \\
            -1 & -1 &  1 \\
        \end{pmatrix},
        Q = \begin{pmatrix}
            1 & 0 & 1 &  -1 \\
            0 & 1 & -2 &  1 \\
            0 & 0 & 1 &  0 \\
            0 & 0 & 0 &  1 \\
        \end{pmatrix}\). We see that
        \begin{align*}
            PAQ &= \begin{pmatrix}
                1 & 0 &  0 \\
                -1 & 1 &  0 \\
                -1 & -1 &  1 \\
            \end{pmatrix}
            \begin{pmatrix}
                1 & 0 & -1 & 1 \\
                1 & 1 & 1 & 0 \\
                2 & 1 & 0 & 1
            \end{pmatrix}
            \begin{pmatrix}
                1 & 0 & 1 &  -1 \\
                0 & 1 & -2 &  1 \\
                0 & 0 & 1 &  0 \\
                0 & 0 & 0 &  1 \\
            \end{pmatrix} \\
            &=\begin{pmatrix}
                1 & 0 & -1 &  1 \\
                0 & 1 & 2 &  -1 \\
                0 & 0 & 0 &  0 \\
            \end{pmatrix}
            \begin{pmatrix}
                1 & 0 & 1 &  -1 \\
                0 & 1 & -2 &  1 \\
                0 & 0 & 1 &  0 \\
                0 & 0 & 0 &  1 \\
            \end{pmatrix}\\
            &=\begin{pmatrix}
                1 & 0 & 0 &  0 \\
                0 & 1 & 0 &  0 \\
                0 & 0 & 0 &  0 \\
            \end{pmatrix}\\
            &= B
        \end{align*}
        as required.
    \end{question}

    \begin{question}{2}
        Let \( A = \begin{pmatrix} 1 & -2 & -4 \\ 1 & 1 & -1 \\ 1 & 0 & -1 \end{pmatrix} \).

        \begin{enumerate}[label=(\alph*)]
            \item Verify that \( A \) is invertible, by row-reducing the augmented matrix \( (A | I_3) \).
            \item Use (a) to find \( A^{-1} \).
            \item Express \( A \) as a product of elementary matrices.
        \end{enumerate}
        \tcblower
        \ 
        
        (a):
        We see that 
        \begin{align*}
            (A|I_3) &= \left( \begin{array}{@{}ccc|ccc@{}}
                1 & -2 & -4 & 1 & 0 & 0 \\
                1 & 1 & -1 & 0 & 1 & 0 \\
                1 & 0 & -1 & 0 & 0 & 1 \\
            \end{array} \right)
            \xrightarrow{r_2\,\to\,r_2 - r_1,r_3\,\to\,r_3 - r_1}
            \left( \begin{array}{@{}ccc|ccc@{}}
                1 & -2 & -4 & 1 & 0 & 0 \\
                0 & 3 & 3 & -1 & 1 & 0 \\
                0 & 2 & 3 & -1 & 0 & 1 \\
            \end{array} \right) \\
            &\xrightarrow{r_1\,\to\,r_1 + r_3,r_2\,\to\,r_2 - r_3}
            \left( \begin{array}{@{}ccc|ccc@{}}
                1 & 0 & -1 & 0 & 0 & 1 \\
                0 & 1 & 0 & 0 & 1 & -1 \\
                0 & 2 & 3 & -1 & 0 & 1 \\
            \end{array} \right)
            \xrightarrow{r_3\,\to\,r_3 - 2r_2}
            \left( \begin{array}{@{}ccc|ccc@{}}
                1 & 0 & -1 & 0 & 0 & 1 \\
                0 & 1 & 0 & 0 & 1 & -1 \\
                0 & 0 & 3 & -1 & -2 & 3 \\
            \end{array} \right) \\
            &\xrightarrow{r_1\,\to\,r_1 + \frac{1}{3}r_3}
            \left( \begin{array}{@{}ccc|ccc@{}}
                1 & 0 & 0 & -\frac{1}{3} & -\frac{2}{3} & 2 \\
                0 & 1 & 0 & 0 & 1 & -1 \\
                0 & 0 & 3 & -1 & -2 & 3 \\
            \end{array} \right)
            \xrightarrow{r_3\,\to\,\frac{1}{3}r_3}
            \left( \begin{array}{@{}ccc|ccc@{}}
                1 & 0 & 0 & -\frac{1}{3} & -\frac{2}{3} & 2 \\
                0 & 1 & 0 & 0 & 1 & -1 \\
                0 & 0 & 1 & -\frac{1}{3} & -\frac{2}{3} & 1 \\
            \end{array} \right)
        \end{align*}
        Since \(A\) can be row reduced into the identity matrix, \(A\) is invertible.

        \medskip

        (b):

        By our row reductions above, we know that \(A^{-1} = \begin{pmatrix}
            -\frac{1}{3} & -\frac{2}{3} & 2 \\
            0 & 1 & -1 \\
            -\frac{1}{3} & -\frac{2}{3} & 1 \\
        \end{pmatrix}\).

        \medskip

        (c):

        To express \(A\) is a product of elementary matrices, we can apply the opposite row operations to the identity matrix in reverse order. That is,
        \[
            A = \begin{pmatrix}
                1 & 0 &  0 \\
                1 & 1 &  0 \\
                0 & 0 &  1 \\
            \end{pmatrix}
            \begin{pmatrix}
                1 & 0 &  0 \\
                0 & 1 &  0 \\
                1 & 0 &  1 \\
            \end{pmatrix}
            \begin{pmatrix}
                1 & 0 &  -1 \\
                0 & 1 &  0 \\
                0 & 0 &  1 \\
            \end{pmatrix}
            \begin{pmatrix}
                1 & 0 &  0 \\
                0 & 1 &  1 \\
                0 & 0 &  1 \\
            \end{pmatrix}
            \begin{pmatrix}
                1 & 0 &  0 \\
                0 & 1 &  0 \\
                0 & 2 &  1 \\
            \end{pmatrix}
            \begin{pmatrix}
                1 & 0 &  -\frac{1}{3} \\
                0 & 1 &  0 \\
                0 & 0 &  1 \\
            \end{pmatrix}
            \begin{pmatrix}
                1 & 0 &  0 \\
                0 & 1 &  0 \\
                0 & 0 &  3 \\
            \end{pmatrix}
        \]
    \end{question}

    \begin{question}{3}
        Find the explicit formula for the linear transformation \( T : \mathbb{Q}^4 \to \mathbb{Q}^3 \) which satisfies:

        \[
        T\begin{pmatrix} 1 \\ 0 \\ 0 \\ 0 \end{pmatrix} = \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}, \quad
        T\begin{pmatrix} 2 \\ 1 \\ 0 \\ 0 \end{pmatrix} = \begin{pmatrix} 0 \\ 1 \\ 1 \end{pmatrix}, \quad
        T\begin{pmatrix} 1 \\ 1 \\ 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}, \quad
        T\begin{pmatrix} 1 \\ 1 \\ 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}.
        \]
    \end{question}

    \begin{question}{4}
        Let \( \mathbb{F} = \mathbb{Q} \) and \( V = \mathcal{M}_{2 \times 2}(\mathbb{F}) \). Consider the linear map \( T : \mathcal{M}_{2 \times 2}(\mathbb{F}) \to \mathcal{M}_{2 \times 2}(\mathbb{F}) \) given by \( T(A) = A^T \). Set \( \beta = \{ E_{11}, E_{12}, E_{21}, E_{22} \} \) and \( \gamma = \{ E_{11}, E_{22}, E_{12} + E_{21}, E_{12} - E_{21} \} \).

        \begin{enumerate}
            \item Find \( P \) - the change of coordinate matrix from \( \gamma \) to \( \beta \) coordinates.
            \item Find \( P^{-1} \) - the change of coordinate matrix from \( \beta \) to \( \gamma \) coordinates.
            \item Find \( A = [T]_\beta \).
            \item Find \( B = [T]_\gamma \).
            \item Confirm that \( A = PBP^{-1} \) using (a)-(d).
        \end{enumerate}
    \end{question}

    \begin{question}{5}
        Let \( T : \mathcal{M}_{n \times n}(\mathbb{F}) \to \mathcal{M}_{n \times n}(\mathbb{F}) \) be the linear map given by \( T(A) = A + A^T \).

        \begin{enumerate}
            \item Find \( N(T) \) and \( \dim N(T) \).
            \item What is \( \operatorname{im}(T) \)?
            \item Is \( \mathcal{M}_{n \times n}(\mathbb{F}) = \operatorname{im}(T) \oplus N(T) \)?
        \end{enumerate}
    \end{question}

    \begin{question}{6}
        Let \( V, W \) be vector spaces over a field \( \mathbb{F} \) and \( T : V \to W \) a linear map. Prove that \( T \) is injective if and only if \( N(T) = \{\mathbf{0}_V\} \). (Make no assumption here about \( \dim V, \dim W \).)
        \tcblower
        Suppose that \(T\) is injective. Let \(T(x) = 0\), for some \(x \in V\). Recall that \(T(0) = 0\) for any linear map. Therefore by injectivity \(x = 0\), so \(N(T) = \{ 0 \}\).

        Conversely, suppose that \(N(T) = \{ 0 \}\). Let \(x,y \in V\) such that \(T(x) = T(y)\). By linearity, we have that \(T(x - y) = 0\), but this implies that \(x - y = 0\), so \(x = y\) and \(T\) is injective.
    \end{question}

    \begin{question}{7}
        Let \( V, W \) be vector spaces over a field \( \mathbb{F} \), and \( T : V \to W \) a linear map. Find a condition on \( T \) which is equivalent to "\( T(S) \) spans \( W \) for any spanning set \( S \subseteq V \) of \( V \)".

        (Hint: Write down the definition of \( T(S) \) is spanning to get started.)
        \tcblower
        We claim that this statement is equivalent to saying that \(T\) is surjective.

        Suppose that for any set \(S \subseteq V\) that spans \(V\), \(T(S)\) spans \(W\). We prove that \(T\) is surjective.

        Let \(w \in W\). We can write \(w\) as a linear combination of some number of vectors in \(T(S)\). That is, for some \(k \in \mathbb{N}\) and \(s_i \in S\), \(c_i \in \mathbb{F}\), \(i \in \{ 1, ..., k \}\),
        \[
            w = \sum_{i=1}^{k} c_i T(s_i) = T \left(\sum_{i=1}^{k}c_i s_i\right)
        \]
        so \(T\) is surjective.

        Conversely, suppose that \(T\) is surjective. Let \(S\) be a spanning set of \(V\). We will show that \(T(S)\) spans \(W\). Let \(w \in W\). By surjectivity, there exists \(v \in V\) so that \(T(v) = w\). We can rewrite
        \[
            v = \sum_{i=1}^{k} c_i s_i
        \]
        for some number of vectors \(s_i \in S\) and \(c_i \in \mathbb{F}\). Then
        \[
            T\left( \sum_{i=1}^{k} c_i s_i \right) = w \implies \sum_{i=1}^{k} c_i T(s_i) = w
        \]
        Notice that \(T(s_i) \in T(S)\), from which it follows that \(T(S)\) spans \(W\), and the proof is complete.
    \end{question}

    \begin{question}{8}
        Let \( P \in \mathcal{M}_{n \times n}(\mathbb{F}) \). Prove the following three conditions are equivalent.

        \begin{enumerate}[label=(\alph*)]
            \item \( P \) is invertible.
            \item There exists bases \( \beta, \gamma \) of \( \mathbb{F}^n \) so that \( P = [\mathrm{I}_{\mathbb{F}^n}]_\beta^\gamma \).
            \item For any \( n \)-dimensional vector space \( V \) over \( \mathbb{F} \), there exists bases \( \beta, \gamma \) of \( V \) so that \( P = [\mathrm{I}_V]_\beta^\gamma \).
        \end{enumerate}
        \tcblower

        Suppose (a). We prove (b).

        Consider the linear transformation \(T_P : \mathbb{F}^n \to \mathbb{F}^n\). Let \(\beta\) be an ordered basis for \(\mathbb{F}^n\). We will show that \(\gamma = T_P(\beta)\) is also an ordered basis for \(\mathbb{F}^n\). Since \(P\) is invertible, \(T_P\) has an inverse \((T_P)^{-1} = T_{P^{-1}}\), so \(T_P\) is surjective and \(\mathrm{span} (T_P(\beta)) = \mathbb{F}^n\). Since \(|T_P(\beta)| = n\), \(T_P(\beta)\) is indeed an ordered basis. Thus we can conclude that \(P\) is a change of basis matrix from \(\beta\) to \(\gamma\).

        \medskip

        Suppose (b). We prove (c).

        Let \(\beta = \{ v_1, ..., v_n \}\) be an ordered basis for \(V\), and
    \end{question}

    \begin{question}{9}
        Consider the relation \( \equiv \) on \( \mathcal{M}_{m \times n}(\mathbb{F}) \) defined by \( A \equiv B \) if \( A \to B \) using a combination of row and/or column operations.

        \begin{enumerate}[label=(\alph*)]
            \item Prove that \( \equiv \) is an equivalence relation on \( \mathcal{M}_{m \times n}(\mathbb{F}) \).
            \item Find a condition on \( A, B \) which is equivalent to \( A \equiv B \). (Hint: Theorem 53.)
            \item Classify the equivalence classes for this relation, and prove that there are exactly \( 1 + \min\{n, m\} \) such classes.
        \end{enumerate}
        \tcblower
        \ 

        (a):

        We show reflexivity, symmetry, and transitivity in that order.

        Reflexivity: Since \(IA = A\), and \(I\) is considered a row operation, \(A \equiv A\).

        Symmetry: Suppose that \(A \equiv B\) then for some invertible matrices \(P,Q\) we have that \(PAQ = B\). But at the same time this means that \(P^{-1} B Q^{-1} = A\) so \(B \equiv A\).

        Transitivity: Suppose that \(A \equiv B\) and \(B \equiv C\). Then for invertible matrices \(P,Q,R,S\), \(PAQ = B\) and \(RBS = C\), so \((RP)A(QS) = R(PAQ)S = RBS = C\). Since \(RP, QS\) are also invertible, we have that \(A \equiv C\).

        \medskip

        (b):

        We claim that an equivalent condition is \(\rank A = \rank B\). Suppose that \(A \equiv B\). Then \(PAQ = B\) for some invertible matrices \(P,Q\), but it is known that rank is preserved by multiplication with invertible matrices, so \(\rank A = \rank PAQ = \rank B\).

        Conversely, suppose that \(r \coloneqq \rank A = \rank B\). By Theorem 53, there exist row/column operations so that
        \[
            A,B \to \left( \begin{array}{@{}c|c@{}}
                I_r & 0 \\
                \hline
                0 & 0
            \end{array} \right).
        \]
        We denote this matrix by \(J_r\).
        that is, for invertible matrices \(P,Q,R,S\), \(PAQ = I' = RBS\). It follows that \(R^{-1} P A Q S^{-1} = B\), so \(A \equiv B\) as desired.

        \medskip

        (c):

        We can classify the equivalence classes by matrix rank. That is, each equivalence class is of the form
        \[
            [J_r] = \{ A \in \mathcal{M} _{m\times n}(\mathbb{F}) : \rank A = r\}.
        \]
        The possible ranks of \(m\times n\) matrices range from \(0\) to \(\min \{ n,m \}\), so there are \(\min \{ n,m \} + 1\) different values of \(r\). We will verify that these equivalence classes are exhaustive and disjoint. Every \(m \times n\) matrix must have a rank, so it belongs to at least one of the classes, but at the same time, a matrix can possibly only have one rank, so it necessarily belongs to exactly one equivalence class.
    \end{question}

    \begin{question}{10}
        Let \( V, W \) be finite dimensional vector spaces over \( \mathbb{F} \), and \( T : V \to W \) a linear map with \(\rank T = 2\). Set \( n = \dim V \), \( m = \dim W \). Let \( \mathbf{x}_1, \mathbf{x}_2 \in \mathbb{F}^n \) be two non-parallel vectors. Prove there exists bases \( \beta, \gamma \) of \( V, W \) respectively, so that \( [T]_\beta^\gamma = (\mathbf{x}_1 \ \mathbf{x}_2 \ \mathbf{0} \ \cdots \ \mathbf{0}) \). (Hint: use problems 7,8.)
    \end{question}

    \begin{question}{11}
        Let \( T : V \to V \) be linear. We say that a subspace \( W \subseteq V \) is ``\( T \)-invariant'' if \( T(W) \subseteq W \).

        For example, if \( T : \mathbb{R}^3 \to \mathbb{R}^3 \) is counter-clockwise rotation around the \( z \)-axis by angle \( \theta \), then \( P_{xy} = \{(x, y, 0) \in \mathbb{R}^3\} \) is \( T \)-invariant, as is \( L_z \) (the \( z \)-axis).

        \begin{enumerate}
            \item Verify the claims made above, by showing that \( P_{xy} \) and \( L_z \) are \( T \)-invariant.
            \item Show that \( \mathbb{R}^3 = P_{xy} \oplus L_z \) by finding a basis \( \beta = \beta_1 \cup \beta_2 \) for \( \mathbb{R}^3 \) so that \( \beta_1 \) is a basis for \( P_{xy} \) and \( \beta_2 \) is a basis for \( L_z \).
            \item Using your basis \( \beta \) from (b), find \( [T]_\beta \).
        \end{enumerate}
    \end{question}

    \begin{question}{12}
        Let \( V \) be a finite dimensional vector space over \( \mathbb{F} \), \( T \in \mathcal{L}(V) \), and \( W_1 \subseteq V \) a \( T \)-invariant subspace with basis \( \beta_1 \). Set \( k = \dim W_1 \).

        We will generalize what we saw in \#11c.

        \begin{enumerate}
            \item Extend \( \beta_1 \) to a basis \( \beta \) of \( V \). Show that \( [T]_\beta = \begin{pmatrix} A & C \\ O_{n-k,k} & B \end{pmatrix} \), where \( A \) is \( k \times k \), \( B \) is \( (n-k) \times (n-k) \), and \( C \) is \( k \times (n-k) \).
            \item Suppose that \( W_2 \) is a subspace so that \( V = W_1 \oplus W_2 \). Let \( \beta = \beta_1 \cup \beta_2 \), where \( \beta_2 \) is any basis for \( W_2 \).

            Prove that if \( W_2 \) is \( T \)-invariant, then \( [T]_\beta = \begin{pmatrix} A & O_{k,n-k} \\ O_{n-k,k} & B \end{pmatrix} \) is block diagonal.
            \item Is the converse of (b) true or false? Justify your answer.
        \end{enumerate}
    \end{question}

    \begin{question}{13}
        Determine if the statements below are true or false. If true, give a proof. If false, explain why, and/or provide a counterexample.

        \begin{enumerate}[label=(\alph*)]
            \item Let \( \beta = \{ e_1, \ldots, e_n \} \) be the standard basis for \( \mathbb{F}^n \), and \( \gamma = \{ v_1, \ldots, v_n \} \) a basis for \( \mathbb{F}^n \). Then there exists a sequence of row operations that takes \( \beta \) to \( \gamma \). (That is, \( v_i \) is obtained from \( e_i \) using the same row operations for all \( i \).)
            \item Let \( V \) be a finite dimensional vector space over \( \mathbb{F} \) and \( T : V \to V \) a linear map. If \( \beta, \gamma \) are bases for \( V \) so that \( [T]_\beta^\gamma = I_n \), then \( T = I_V \).
            \item Let \( V \) be a finite dimensional vector space over \( \mathbb{F} \) and \( S, T : V \to V \) linear maps. If rank \( T = \text{rank } S \), then there exist bases \( \beta, \beta', \gamma, \gamma' \) for \( V \) so that \( [S]_\beta^\gamma = [T]_\beta^\gamma \).
            \item Let \( A, B \in \mathcal{M}_{n \times n}(\mathbb{F}) \). If \( A^2 \sim B^2 \), then \( A \sim B \).
        \end{enumerate}
    \end{question}
\end{document}