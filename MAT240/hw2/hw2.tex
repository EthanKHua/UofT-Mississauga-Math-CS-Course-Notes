\documentclass{article}
\usepackage[margin=1.0in]{geometry}
\usepackage{amssymb,amsmath,amsthm,amsfonts}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{systeme}

% My boxes
\usepackage[breakable]{tcolorbox}

% \RequirePackage{background}
% \backgroundsetup{
%     scale=1,
%     color=black,
%     opacity=1,
%     angle=0,
%     contents={
%         \includegraphics[width=\paperwidth,height=\paperheight]{\nightmodebackground}
%     }
% }

\definecolor{pastelblue}{RGB}{96, 145, 245}
\definecolor{pastelgreen}{RGB}{106, 235, 135}
\definecolor{darkgray}{RGB}{60, 60, 60}
\definecolor{lightgray}{RGB}{180, 180, 180}
\definecolor{offwhite}{RGB}{225, 225, 245}


\pagecolor{darkgray}
\color{offwhite}

\newcommand{\Z}{\mathbf{Z}}
\newcommand{\N}{\mathbf{N}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\C}{\mathbf{C}}

\newcommand{\id}{\mathrm{id}}
\newcommand{\op}{\mathrm{op}}
\newcommand{\diam}{\mathrm{diam}}
\newcommand{\GL}{\mathrm{GL}}
\newcommand{\Tr}{\mathrm{Tr}}
\newcommand{\im}{\mathrm{im}}
\newcommand{\rank}{\mathrm{rank}}

\newcommand{\cl}[1]{\overline{#1}}

\newenvironment{amatrix}[1]{%
  \left(\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right)
}

\swapnumbers % places numbers before thm names

\theoremstyle{plain} % The "plain" style italicizes all body text.
	\newtheorem{thm}{Theorem}
		\numberwithin{thm}{section} % Theorem numbers are determined by section.
	\newtheorem{lemma}[thm]{Lemma}
	\newtheorem{prop}[thm]{Proposition}
	\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
    \newtheorem{defn}[thm]{Definition}
	\newtheorem{example}[thm]{Example}
	\newtheorem{exercise}[thm]{Exercise} %Exercise

\begin{document}
    \newtcolorbox{question}[2][]{fonttitle=\large, fontupper=\large, fontlower=\large, title=Question {#2}., oversize, arc=3mm, outer arc=2mm, opacityback=0.9, coltitle=offwhite, colframe=pastelblue, colback=darkgray, colupper=lightgray, collower=lightgray, leftrule=1mm, rightrule=1mm, toprule=1.5mm, titlerule=1mm, bottomrule=1mm, valign=center, add to natural height=5mm, lower separated=false, before lower=\begin{proof}, after lower= \smallbreak \end{proof}, #1, breakable=true}

    \begin{question}{1}
        Let \( V \) be a vector space over the field \( \mathbb{F} \), and \( S \) a (non-empty) set. Let \( \mathcal{F}(S,V) = \{ f : S \to V \} \) be the set of \( V \)-valued functions.
        
        We define addition and scaling on \( \mathcal{F}(S,V) \) pointwise:
        \[
        (f + g)(s) = f(s) + g(s)
        \]
        \[
        (cf)(s) = cf(s)
        \]
        
        We will verify some of the vector space axioms required to prove that \( \mathcal{F}(S,V) \) is a vector space over \( \mathbb{F} \).
        
        \begin{enumerate}[label=(\alph*)]
            \item Why do these operations make sense?
            \item Prove (using only the definitions above, and the fact that \( V \) is a vector space) that \( c(f + g) = cf + cg \) for all \( f,g \in \mathcal{F}(S,V) \) and \( c \in \mathbb{F} \).
            \item Prove that for all \( f \in \mathcal{F}(S,V) \) there exists \( g \in \mathcal{F}(S,V) \), so that \( f + g = 0 \). (Here \( 0 : S \to V \) is the constant function defined by \( 0(s) = 0_V \) for \( s \in S \).)
        \end{enumerate}

        \tcblower
        \ 

        (a):

        To show that these operations are well defined, notice that \(f(s), g(s) \in V\), so by the axiom of closure on \(V\) we have that
        \[
            (f+g)(s) = f(s) + g(s) \in V \text{ and } (cf)(s) = cf(s) \in V
        \]

        \medskip

        (b):

        We will do this by showing that for all \(s \in S\), we have \(c(f(s) + g(s)) = cf(s) + cg(s)\).

        Fix \(s \in S\). It follows that \(f(s), g(s) \in V\), so by the axiom of distributivity in \(V\), we have that
        \[
            c(f(s) + g(s)) = cf(s) + cg(s)
        \]

        \medskip

        (c):

        Let \(f \in \mathcal{F} (S,V)\). Choose \(g = (-1 \cdot f)\). Then for all \(s \in S\),
        \[
            f(s) + g(s) = f(s) + (-f(s)) = 0
        \]
        as needed.
    \end{question}
    \newpage
    \begin{question}{2}
        Let \( W = \left\{ (x,y,z,w) \in \mathbb{Q}^4 \middle| \begin{array}{l}
        x + 5w = y + 5z \\
        y = 4w - 3z \\
        x + y + z = 3w
        \end{array} \right\} \).
        
        Do not use Q3 to solve this problem. This problem is a “warm up” for Q3.
        
        \begin{enumerate}[label=(\alph*)]
            \item Rearrange the equations defining \( W \) to show that \( W \) is the set of solutions to a homogeneous system of equations.
            \item Solve the system using row-reduction and express the general solution as a linear combination of the “basic solutions”.
            \item Show that \( W = \text{span} \, S \), for some set \( S \subseteq \mathbb{Q}^4 \).
            \item Deduce that \( W \) is a subspace of \( \mathbb{Q}^4 \).
        \end{enumerate}

        \tcblower
        \ 

        (a):

        Rearranging, the equations become
        \begin{equation*}
            \systeme[xyzw]{
                x - y - 5z + 5w = 0,
                y + 3z -4w = 0,
                x + y + z -3w = 0
            }
        \end{equation*}

        \medskip

        (b):

        The augmented matrix associated with this system of equations is
        \[
            \begin{amatrix}{4}
                1 & -1 & -5 & 5 & 0 \\
                0 & 1 & 3 & -4 & 0 \\
                1 & 1 & 1 & -3 & 0
            \end{amatrix}
        \]
        Row reducing this, we get
        \begin{align*}
            \begin{amatrix}{4}
                1 & -1 & -5 & 5 & 0 \\
                0 & 1 & 3 & -4 & 0 \\
                1 & 1 & 1 & -3 & 0
            \end{amatrix}
            \xrightarrow{r_3\,\to\,r_3 - r_1}
            \begin{amatrix}{4}
                1 & -1 & -5 & 5 & 0 \\
                0 & 1 & 3 & -4 & 0 \\
                0 & 2 & 6 & -8 & 0
            \end{amatrix}
            \xrightarrow[r_3\,\to\,r_3 - 2r_2]{r_1\,\to\,r_1 + r_2}
            \begin{amatrix}{4}
                1 & 0 & -2 & 1 & 0 \\
                0 & 1 & 3 & -4 & 0 \\
                0 & 0 & 0 & 0 & 0
            \end{amatrix}
        \end{align*}
        We parameterize \(z\) and \(w\) to obtain that
        \begin{align*}
            x &= 2s - t \\
            y &= -3s + 4t \\
            z &= s \\
            w &= t
        \end{align*}
        so the general solution of this system of equations is given by
        \[
            \begin{pmatrix*}
                 x \\
                 y \\
                 z \\
                 w \\
            \end{pmatrix*}
            =
            s\begin{pmatrix*}[r]
                 2 \\
                 -3 \\
                 1 \\
                 0 \\
            \end{pmatrix*}
            +t
            \begin{pmatrix*}[r]
                 -1 \\
                 4 \\
                 0 \\
                 1 \\
            \end{pmatrix*}
        \]

        \medskip

        (c):

        Let
        \(S = \left\{ \begin{pmatrix*}[r]
                        2 \\
                        -3 \\
                        1 \\
                        0 \\
                      \end{pmatrix*},
                      \begin{pmatrix*}[r]
                        -1 \\
                        4 \\
                        0 \\
                        1 \\
                      \end{pmatrix*}\right\} \).
        Let \(\vec{v} \in \mathrm{span} S\). It follows that
        \[
            \vec{v} = s\begin{pmatrix*}[r]
                2 \\
                -3 \\
                1 \\
                0 \\
           \end{pmatrix*}
           +t
           \begin{pmatrix*}[r]
                -1 \\
                4 \\
                0 \\
                1 \\
           \end{pmatrix*}
        \]
        for some \(s,t \in \mathbb{Q}\). But notice that this is actually a solution to the system in \(W\), so \(\vec{v} \in W\).

        Now let \(\vec{w} \in W\), so \(\vec{w}\) solves the system of equations in \(W\), but this means that we can write \(\vec{w}\) as a linear combination of the vectors in \(S\), so \(\vec{w} \in \mathrm{span} S\), so \(W = \mathrm{span} S\).

        \medskip

        (d):

        By the previous part, \(W\) is actually a spanning set, and we know that all spanning sets are subspaces, so we conclude that \(W\) is a subspace of \(\mathbb{Q}^4\).
    \end{question}
    \newpage
    \begin{question}{3}
        We now generalize Q2. Consider a linear system with \( m \) equations and \( n \) unknowns:
        
        \[
        \begin{aligned}
        a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= 0 \\
        a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &= 0 \\
        &\vdots \\
        a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &= 0.
        \end{aligned}
        \]
        
        We saw in Week 3 that any solution \( x = (x_1, x_2, \ldots, x_n) \in \mathbb{F}^n \) can be expressed as \( x = \sum_{i=1}^{k} t_i x_i \), where \( t_i \in \mathbb{F} \) are the parameters, and \( x_i \in \mathbb{F}^n \) are the “basic solutions”.
        
        Let \( W \) be the set of solutions to this system.
        
        \begin{enumerate}[label=(\alph*)]
            \item Prove that \( W = \text{span} \, S \) for some set \( S \), and hence that \( W \) is a subspace of \( \mathbb{F}^n \).
            \item Prove that the set \(\{x_1, x_2, \ldots, x_k\}\) is linearly independent.
            
            (Hint: Think about the variables which correspond to the choice of parameters. There is exactly one vector for each such parameter. Use the corresponding entry to show that if \( t_1 x_1 + t_2 x_2 + \cdots + t_k x_k = 0 \) then \( t_i = 0 \) for each \( i \).)
            
            \item Find a basis for \( W \).
        \end{enumerate}

        \tcblower
        \ 

        (a):

        Let \(S = \{ x_1, ..., x_k \}\). We note that any linear conbination of the vectors in \(S\) form a solution to the system, but not only that, any solution to the system can be written as a linear combination of these vectors, so \(\mathrm{span} S = W\).

        \medskip

        (b):

        Suppose that \(\sum_{i=1}^{k} t_i x_i = 0\).

        \medskip

        (c):

        From part (a) and part (b), the set \(S = \{ x_1, ..., x_k \}\) spans \(W\) and is linearly independent, which by definition forms a basis of \(W\).
    \end{question}
    \newpage
    \begin{question}{4}
        Is the set \( S = \{e_1 + 2 e_2 - 3 e_3, e_1 + e_2 - e_3, e_2 - e_3\} \subseteq \mathbb{Q}^3 \) a basis for \( \mathbb{Q}^3 \)? Justify your answer.

        \tcblower

        We claim that \(S\) is indeed a basis for \(\mathbb{Q}^3\). For convenience, denote the vectors in \(S\) by \(v_1, v_2, v_3\) respectively. Notice that
        \[
            e_1 = v_2 - v_3,\ e_2 = -v_1 + v_2 + 2v_3,\ e_3 = -v_1 + v_2 + v_3.
        \]
        Thus, for any \(x \in \mathbb{Q}^3\), since \(\{ e_1, e_2, e_3 \}\) is a basis for \(\mathbb{Q}^3\), for some \(a,b,c \in \mathbb{Q}\), we have that
        \[
            x = ae_1 + be_2 + ce_3 = a(v_2 - v_3) + b(-v_1 + v_2 + 2v_3) + c(-v_1 + v_2 + v_3)
        \]
        \[
            \implies x = (-b - c)v_1 + (a + b + c)v_2 + (-a + c)v_3
        \]
        which shows that \(S\) spans \(\mathbb{Q}^3\).

        Now, for constants \(p,q,r \in \mathbb{Q}\), suppose that
        \[
            0 = pv_1 + qv_2 + rv_3
        \]
        SUbstituting back our values, we get that
        \begin{align*}
            0 &= p(e_1 + 2e_2 - 3e_3) + q(e_1 + e_2 - e_3) + r(e_2 - e_3) \\
            &= (p + q)e_1 + (2p + q + r)e_2 + (-3p -q -r)e_3
        \end{align*}
        By the linear independence of the standard vectors, we have that
        \begin{equation*}
            \sysdelim..
            \systeme{
                p+q = 0,
                2p + q + r = 0,
                -3p -q - r = 0
            }
        \end{equation*}
        We can solve for \(p,q,r\) to get that \(p = q = r = 0\).

        Thus we can conclude that \(S\) is a basis for \(\mathbb{Q}^3\).
    \end{question}
    \newpage
    \begin{question}{5}
        Let \( V \) be a finite dimensional vector space over a field \( \mathbb{F} \).
        
        \begin{enumerate}[label=(\alph*)]
            \item Prove that if \( W \subseteq V \) is a subspace with basis \( \beta_W \), then there exists a linearly independent set \( \alpha \) so that \( \beta = \beta_W \cup \alpha \) is a basis for \( V \). (We say that \( \beta \) “extends” \( \beta_W \). So you are proving that “every basis of a subspace \( W \) can be extended to a basis of \( V \)”.)
            \item Prove that for any linearly independent set \( I \) and spanning set \( S \), we have \( |I| \leq \dim V \leq |S| \).
        \end{enumerate}

        \tcblower
        \ 

        (a):

        Let \(\gamma\) be a basis for \(V\). Since \(\beta _W\) is linearly independent, we apply the Replacement Theorem to get that there exists a subset \(\alpha \subseteq \gamma\) such that \(\beta _W \cup \alpha\) is a basis for \(V\), and we are done.

        \medskip

        (b):

        Let \(I\) be a linearly independent set. Then \(W = \mathrm{span} (I)\) is a subspace of \(V\) with basis \(I\). By part (a), we can extend \(I\) to a basis \(\beta\) of \(V\), where \(\beta = I \cup \alpha\). Since \(|\alpha | \geq 0\), we have that
        \[
            \dim V = |\beta| = |I| + |\alpha| \geq |I|
        \]
        Now, let \(S\) be a spanning set of \(V\). If \(\dim V = 0\), then \(V = \{ 0 \}\) and its basis is \(\beta = \varnothing\). \(S\) must be either \(\varnothing\) or \(\{ 0 \}\), so \(\dim V \leq |S|\).

        If \(\dim V > 0\), it contains a non-zero vector, so \(S\) also contains a non-zero vector. Pick \(s_0 \in S\), and note that \(S_0 = \{ s_0 \}\) is linearly independent.
        
        If there are no elements \(w_i\) in \(S\) such that \(\{ s_1, w_i \}\) is linearly independent, that is, \(s = c_i w_i\) for some \(c_i \in \mathbb{F}\), then for all \(v \in V\), because \(S\) is a spanning set, for \(m\) vectors in \(S\) we have that
        \[
            v = \sum_{i=1}^{m} a_i w_i = \sum_{i=1}^{m} a_i \cdot c_i s_0
        \]
        which implies that \(\mathrm{span} (\{ s_0 \} ) = V\), and the result that we want follows immediately after.

        If not, we can find another non-zero vector \(s_1\) so that \(S_1 = \{ s_0, s_1 \}\) is linearly independent. We repeat this process until we have a linearly independent set \(S_n = \{ s_0, ..., s_{n} \}\), where \(n = \dim V\). We claim that \(S_n\) is a basis for \(V\), and it suffices to show that \(S_n\) spans \(V\). First, if \(S_n = S\), then our result is immediate. Otherwise, let \(s_j \in S\) so that \(s_j \notin S_n\). Consider the set \(S_n \cup \{ s_j \}\), whose number of elements is greater than \(\dim V\). By the first half of this proof, we know that no linearly independent set can have a size larger than \(\dim V\), so it must be true that \(S_n \cup \{ s_j \}\) is linearly dependent. In particular, for constants \(a_j,a_{ij} \in \mathbb{F}\) not all zero,
        \[
            0 = a_js + \sum_{i=1}^{n} a_{ij} s_{i}.
        \]
        Notice that \(a_j \neq 0\), for if not, then we get that
        \[
            0 = \sum_{i=1}^{n} a_{ij} s_{i}
        \]
        which implies that every \(a_{ij} = 0\): a contradiction. Thus we can rearrange for \(s\) to obtain
        \[
            s = a_j^{-1} \sum_{i=1}^{n} a_{ij} s_{i}
        \]
        Now, let \(v \in V\). Since \(S\) spans \(V\), we have that for \(m\) vectors \(s_j \in S\) and non-zero \(c_j \in \mathbb{F}\),
        \[
            v = \sum_{j=1}^{m} c_j s_j = \sum_{j=1}^{m} \left( c_j \cdot a_j^{-1} \sum_{i=1}^{n} a_{ij} s_{i} \right)
        \]
        which shows that \(v\) is a linear combination of the vectors in \(S_n\), so \(S_n\) is a linearly independent spanning set for \(V\). Therefore we can conclude that \(S_n\) is a basis for \(V\) and we are done.
    \end{question}
    \newpage
    \begin{question}{6}
        Consider a matrix \( M \in \mathcal{M}_{n \times n}(\mathbb{F}) \). Given \( p \in \{1, \ldots, n\} \) we can split \( M \) into “blocks”: 
        
        \[
        M = \left( \begin{array}{c|c}
            A & B \\
            \hline
            C & D
        \end{array} \right) 
        \]
        
        where \( A \) is \( k \times k \), \( B \) is \( k \times (n-k) \), \( C \) is \( (n-k) \times k \) and \( D \) is \( (n-k) \times (n-k) \).
        
        For example, if \( n = 5 \) and \( k = 2 \), then such a block matrix would be of the form
        
        \[
        M = \left(\begin{array}{cc|ccc} 
        1 & 2 & 3 & 2 & 3 \\ 
        -5 & 3 & 3 & 1 & 1 \\
        \hline 
        1 & 2 & 0 & -1 & 1 \\ 
        3 & 1 & 3 & -1 & 7 \\ 
        1 & 0 & -1 & 3 & 5 
        \end{array}\right)
        \]
        
        where \( A = \begin{pmatrix} 
        1 & 2 \\
        -5 & 3 
        \end{pmatrix}, B = \begin{pmatrix} 
        3 & 2 & 3 \\ 
        3 & 1 & 1 
        \end{pmatrix}, C = \begin{pmatrix} 
        1 & 2 \\ 
        3 & 1 \\ 
        1 & 0 
        \end{pmatrix}, D = \begin{pmatrix} 
        0 & -1 & 1 \\ 
        3 & -1 & 7 \\ 
        -1 & 3 & 5 
        \end{pmatrix} \).
        
        Prove that if \( M = \left(\begin{array}{c|c} 
        A & B \\ 
        \hline
        C & D 
        \end{array}\right) \) and \( N = \left(\begin{array}{c|c} 
        A' & B' \\
        \hline
        C' & D' 
        \end{array}\right) \), then
        \[ \alpha M + N = \left(\begin{array}{c|c}
        \alpha A + A' & \alpha B + B' \\
        \hline
        \alpha C + C' & \alpha D + D' 
        \end{array}\right) \]

        \tcblower


    \end{question}
    \newpage
    \begin{question}{7}
        Let \( W = \left\{ A \in \mathcal{M}_{2n \times 2n}(\mathbb{F}) \mid A = \left( \frac{X - X^T}{O_n} \middle| \frac{O_n}{X + X^T} \right) \text{ with } X \in \mathcal{M}_{n \times n}(\mathbb{F}) \right\} \).
        
        (Assume char\((\mathbb{F})\neq 2\).)
        
        \begin{enumerate}[label=(\alph*)]
            \item Let \( n = 2 \). Find a basis for \( W \).
            \item Now generalize to arbitrary \( n \). Find a basis for \( W \), and use it to compute dim \( W \).
        \end{enumerate}

        \tcblower
        \ 

        (a):

        Let \(n = 2\). We claim that a basis for \(W\) is given by
        \[
            \beta = \left\{ \begin{pmatrix}
                0 & 0 & 0 &  0 \\
                0 & 0 & 0 &  0 \\
                0 & 0 & 1 &  0 \\
                0 & 0 & 0 &  0 \\
            \end{pmatrix},
            \begin{pmatrix}
                0 & 1 & 0 &  0 \\
                -1 & 0 & 0 &  0 \\
                0 & 0 & 0 &  1 \\
                0 & 0 & 1 &  0 \\
            \end{pmatrix},
            \begin{pmatrix}
                0 & -1 & 0 &  0 \\
                1 & 0 & 0 &  0 \\
                0 & 0 & 0 &  1 \\
                0 & 0 & 1 &  0 \\
            \end{pmatrix},
            \begin{pmatrix}
                0 & 0 & 0 &  0 \\
                0 & 0 & 0 &  0 \\
                0 & 0 & 0 &  0 \\
                0 & 0 & 0 &  1 \\
            \end{pmatrix}\right\}.
        \]
        For convenience, we label these matrices in order as \(f_1, f_2, f_3\), and \(f_4\). It is pretty clear that \(\beta\) is linear independent. To show that \(\beta\) is spanning, let \(A \in W\). Then
        \[
            A = \begin{amatrix}{1}
                X - X^T & 0_n \\
                \hline \\[-10pt]
                0_n & X + X^T
            \end{amatrix}, \text{ for some } X = \begin{pmatrix}
                a &  b \\
                c &  d \\
            \end{pmatrix} \in \mathcal{M} _{n\times n}(\mathbb{F}).
        \]
        We substitute \(a,b,c,d\) to get
        \begin{align*}
            A &= \begin{pmatrix}
                0 & b - c & 0 &  0 \\
                c - b & 0 & 0 &  0 \\
                0 & 0 & 2a &  b + c \\
                0 & 0 & b + c &  2d \\
            \end{pmatrix}
            = 2af_1 + bf_2 + cf_3 + 2df_4
        \end{align*}
        so we can conclude that \(W\) is spanned by \(\beta\).

        \medskip

        (b):

        Let \(n \in \mathbb{N}\). We define \(e_{ij}\) to be the \(n\times n\) matrix with all its entries equal to 0 except the entry at the \(i\)th row and \(j\)th column. We claim that
        \[
            \beta = \left\{ \begin{amatrix}{1}
                e_{ij} - e_{ij}^T & 0_n \\
                \hline \\[-13pt]
                0_n & e_{ij} - e_{ij}^T
            \end{amatrix}, 1 \leq i,j \leq n\right\} 
        \]
        is a basis for \(W\).

        Similarly to the previous part, define each matrix associated with \(e_{ij}\) by \(f_{ij}\). Let \(A \in W\), so
        \[
            A = \begin{amatrix}{1}
                X - X^T & 0_n \\
                \hline \\[-10pt]
                0_n & X + X^T
            \end{amatrix}, \text{ for an } n\times n \text{ matrix } X.
        \]
        We can write \(X\) in terms of the basis for \(\mathcal{M} _{n\times n}(\mathbb{F})\) to obtain that
        \[
            X = \sum_{i,j=1}^{n} c_{ij} e_{ij}.
        \]
        By the linearity of matrix transposition, it follows that
        \begin{align*}
            A &= \begin{amatrix}{1}
                \sum_{i,j=1}^{n} c_{ij} e_{ij} - \left( \sum_{i,j=1}^{n} c_{ij} e_{ij} \right)^T & 0_n \\
                \hline \\[-10pt]
                0_n & \sum_{i,j=1}^{n} c_{ij} e_{ij} + \left( \sum_{i,j=1}^{n} c_{ij} e_{ij} \right)^T
            \end{amatrix} \\
            &= \begin{amatrix}{1}
                \sum_{i,j=1}^{n} c_{ij} e_{ij} -\sum_{i,j=1}^{n} c_{ij} e_{ij}^T & 0_n \\
                \hline \\[-10pt]
                0_n & \sum_{i,j=1}^{n} c_{ij} e_{ij} + \sum_{i,j=1}^{n} c_{ij} e_{ij}^T \\
            \end{amatrix} \\
            &= \sum_{i,j=1}^{n} c_{ij}\begin{amatrix}{1}
                e_{ij} - e_{ij}^T & 0_n \\
                \hline \\[-10pt]
                0_n &  e_{ij} + e_{ij}^T \\
            \end{amatrix} \\
            &= \sum_{i,j=1}^{n} c_{ij} f_{ij}
        \end{align*}
        so \(W\) is spanned by \(\beta\).

        To show independence, for constants \(c_{ij} \in \mathbb{F}\), let
        \[
            \sum_{i,j=1}^{n} c_{ij} f_{ij} = 0
        \]
        We get that
        \begin{align*}
            \sum_{i,j=1}^{n} c_{ij}\begin{amatrix}{1}
                e_{ij} - e_{ij}^T & 0_n \\
                \hline \\[-10pt]
                0_n &  e_{ij} + e_{ij}^T \\
            \end{amatrix} = 0
        \end{align*}
        In order for this to be true, we must have that
        \[
            \sum_{i,j=1}^{n} c_{ij} (e_{ij} - e_{ij}^T) = 0
        \]
        and
        \[
            \sum_{i,j=1}^{n} c_{ij} (e_{ij} + e_{ij}^T) = 0
        \]
        We add both equations together to see that
        \[
            2 \sum_{i,j=1}^{n} c_{ij} e_{ij} = 0.
        \]
        Since the \(e_{ij}\)'s form a basis on \(\mathcal{M} _{n\times n}(\mathbb{F})\), it follows that \(c_{ij} = 0\) for all \(i,j\), and we are done.
    \end{question}
    \newpage
    \begin{question}{8}
        \begin{enumerate}[label=(\alph*)]
            \item Prove that if \( W_1, W_2 \subseteq V \) are subspaces, then \( W_1 + W_2 \) is a subspace.
            \item Let \( W_1 = \{(x, y, x + y) \in \mathbb{F}^3 \mid x, y \in \mathbb{F} \} \). Find two subspaces \( W_2, W_3 \) so that:
            
            \begin{itemize}
                \item \( W_1 + W_2 = \mathbb{F}^3 \) but \( \mathbb{F}^3 \neq W_1 \oplus W_2 \).
                \item \( W_1 \oplus W_3 = \mathbb{F}^3 \).
            \end{itemize}
            
            \item Find another subspace \( U \subseteq \mathbb{F}^3 \) so that \( W_1 \oplus U = \mathbb{F}^3 \).
        \end{enumerate}

        \tcblower
        \ 

        (a):

        Let \(W_1, W_2\) be subspaces of \(V\). We verify that \(W_1 + W_2\) is also a subspace of \(V\).

        First, note that \(0 \in W_1, W_2\), so \(0 + 0 = 0 \in W_1 + W_2\). Next, let \(c \in \mathbb{F}\), \(\vec{v} , \vec{w} \in W_1 + W_2\). Then \(\vec{v} = \vec{v}_1 + \vec{v}_2\) and \(\vec{w} = \vec{w}_1 + \vec{w}_2\), for some \(\vec{v}_1, \vec{w}_1 \in W_1\) and \(\vec{v}_2, \vec{w}_2 \in W_2\). Since \(W_1, W_2\) are subspaces, it is true that
        \[
            c\vec{v}_1 + \vec{w}_1 \in W_1 \text{ and } c\vec{v}_2 + \vec{w}_2 \in W_2
        \]
        which implies that
        \[
            \vec{v} + \vec{w} = (c\vec{v}_1 + \vec{w}_1) + (c\vec{v}_2 + \vec{w}_2) \in W_1 + W_2,
        \]
        verifying that \(W_1 + W_2\) is indeed a subspace.

        \medskip

        (b):

        Let \(W_2 = \mathbb{F}^3\), \(W_3 = \mathrm{span} \{e_3\}\). We start by showing that \(\mathbb{F}^3 = W_1 + W_2\). The backward direction is instant so we will only show that \(\mathbb{F}^3 \subseteq W_1 + W_2\).

        Let \(x \in \mathbb{F}^3\). We know that \(x\) is the same as \(0 + x\), and \(0 \in W_1\) and \(x \in W_2\), so \(x \in W_1 + W_2\). However, \(W_1 \subseteq W_2\), so \(W_1 \cap W_2 = W_1 \neq \{ 0 \}\), so \(\mathbb{F}^3\) is not a direct sum of \(W_1\) and \(W_2\).

        Now, we will show that \(W_1 \oplus W_3 = \mathbb{F}^3\). Again, the fact that \(W_1 + W_3 \subseteq \mathbb{F}^3\) is obvious. To show that \(\mathbb{F}^3 \subseteq W_1 + W_3\), let \((x,y,z) \in \mathbb{F}^3\). Notice that
        \[
            (x,y,z) = (x,y,x+y) + (0, 0, z - x - y)
        \]
        and
        \[
            (x,y,x+y) \in W_1 \text{ and } (0,0,z-x-y) \in W_3,
        \]
        so \((x,y,z) \in W_1 + W_3\).

        Now let \((a,b,c) \in W_1 \cap W_3\). Then we have that \(a = b = 0\), but this implies that \(c = a + b = 0\), so \((a,b,c) = 0\). Thus \(W_1 \cap W_3 = \{ 0 \}\) and we can conclude that \(\mathbb{F}^3 = W_1 \oplus W_2\).

        \medskip

        (c):

        Let \(U = \mathrm{span} \{ (0,1,2) \}\). We follow the same structure as before in part (b).

        Let \((x,y,z) \in \mathbb{F}^3\). We rewrite
        \[
            (x,y,z) = \left( x, x +2y-z, 2x+2y-z\right) + \left( 0,  - x - y + z, 2(- x - y + z)\right) 
        \]
        and note that \(\left( x, x +2y-z, 2x+2y-z\right) \in W_1\) and \(\left( 0,  - x - y + z, 2(- x - y + z)\right) \in W_2\), so \((x,y,z) \in W_1 + W_2\).

        Now, let \((a,b,c) \in W_1 \cap W_2\). Since \((a,b,c) \in W_2\), it must be true that \(a = 0\) and \(2b = c\). At the same time, since \((a,b,c) \in W_1\) we have that \(a + b = c\), which implies that \(b=c\), which can only be true if \(b = c = 0\), so \((a,b,c) = 0\), thus showing that \(W_1 \oplus W_2 = \mathbb{F}^3\).
    \end{question}
    \newpage
    \begin{question}{9}
        Let \( V \) be a finite dimensional vector space over \( \mathbb{F} \), and \( W_1, W_2 \subseteq V \) subspaces with mutually disjoint bases \( \beta_1, \beta_2 \) respectively. Prove that \( V = W_1 \oplus W_2 \) if and only if \( \beta = \beta_1 \cup \beta_2 \) is a basis for \( V \).

        \tcblower

        Let \(m = |\beta_1|\), \(k = |\beta _2|\).

        Suppose that \(V = W_1 \oplus W_2\). We will show that \(\beta = \beta _1 \cup \beta _2\) is a basis for \(V\).

        Let \(x \in V\). By our assumption, \(x = w_1 + w_2\), for some \(w_1 \in W_1\) and \(w_2 \in W_2\). These vectors can in turn be written as
        \[
            w_1 = \sum_{i=1}^m a_i v_i \text{ and } w_2 = \sum_{i=1} ^k b_i w_i
        \]
        where \(v_i \in \beta _1\) and \(w_i \in \beta _2\). Thus \(x\) can be written as a linear combination of vectors in \(\beta\):
        \[
            x = \sum_{i=1}^m a_i v_i + \sum_{i=1} ^k b_i w_i
        \]
        so \(\beta\) spans \(V\).

        To show that \(\beta\) is linearly independent, suppose that
        \[
            \sum_{i=1}^{m} a_i v_i + \sum_{i=1}^{k} b_i w_i = 0
        \]
        We put the vectors of each subspace on each side to get
        \[
            \sum_{i=1}^{m} a_i v_i = - \sum_{i=1}^{k} b_i w_i
        \]
        By the closure property of subspaces, \(\sum_{i=1}^{m} a_i v_i \in W_1\) and \(\sum_{i=1}^{k} b_i w_i \in W_2\), but since they are equal, it must be true that \(\sum_{i=1}^{m} a_i v_i = \sum_{i=1}^{k} b_i w_i \in W_1 \cap W_2 = \{ 0 \}\), so \(\sum_{i=1}^{m} a_i v_i = \sum_{i=1}^{k} b_i w_i = 0\). Since \(\beta _1, \beta _2\) are linearly independent, it must be true that \(a_i = 0\) and \(b_i = 0\), which was what we wanted to show. Therefore \(\beta\) is indeed a basis for \(V\).

        Conversely, suppose that \(\beta\) is a basis for \(V\). We want to show that \(V = W_1 \oplus W_2\). It is obvious that \(W_1 + W_2 \subseteq V\), so it suffices to prove that \(V \subseteq W_1 \oplus W_2\) and \(W_1 \cap W_2 = \{ 0 \}\).

        Let \(x \in V\). Then since \(\beta\) is a basis, we have that
        \[
            x = \sum_{j=1}^{m} a_i v_i + \sum_{j=1}^{k} b_i w_i, \text{ for } a_i, b_i \in \mathbb{F}, v_i \in \beta _1, \text{ and } w_i \in \beta _2.
        \]
        By closure, we have that \(\sum_{j=1}^{m} a_i v_i \in W_1\) and \(\sum_{j=1}^{k} b_i w_i \in W_2\), so we see that \(x \in W_1 + W_2\). Thus \(V = W_1 + W_2\).

        To show that \(W_1 \cap W_2 = \{ 0 \}\), it suffices to show that if \(x \in W_1 \cap W_2\), then it must be true that \(x = 0\). Indeed, if \(x \in W_1 \cap W_2\), we can write it as a two linear combinations of vectors in either \(\beta _1\) or \(\beta _2\):
        \[
            x = \sum_{i=1}^{m} a_i v_i = \sum_{i=1}^{k} b_i w_i
        \]
        \[
            \implies \sum_{i=1}^{m} a_i v_i - \sum_{i=1}^{k} b_i w_i = 0
        \]
        By the linear independence of \(\beta\), we have that \(a_i = b_i = 0\), for all \(i\), which means that \(x = 0\), as desired, and the proof is complete.
    \end{question}
    \newpage
    \begin{question}{10}
        Let \( J = \left( \begin{array}{c|c} O & -I_2 \\ \hline I_2 & O \end{array} \right) \) and \( \mathbb{F} = \mathbb{C} \).
        
        \begin{enumerate}[label=(\alph*)]
            \item Verify that \( J^2 = -I_4 \).
            \item Find all \( X \in \mathcal{M}_{4\times 4}(\mathbb{F}) \) so that \( XJ = JX \).
            \item Show that \( \mathfrak{sp}_4 = \{ X \in \mathcal{M}_{4\times 4}(\mathbb{F}) | XJ = JX \} \) is a subspace of \( \mathcal{M}_{4\times 4}(\mathbb{F}) \).
            \item Find dim \( \mathfrak{sp}_4 \) by finding a basis for \( \mathfrak{sp}_4 \).
        \end{enumerate}
        \tcblower
        \ 

        (a):

        Indeed, we have that
        \[
            J^2 = \left( \begin{array}{c|c} O & -I_2 \\ \hline I_2 & O \end{array} \right) \left( \begin{array}{c|c} O & -I_2 \\ \hline I_2 & O \end{array} \right) = \left( \begin{array}{c|c} O^2 + -I_2^2 & O(-I_2) + -I_2 O \\ \hline OI_2 + I_2 O & - I_2^2 + O^2 \end{array} \right) = \left( \begin{array}{c|c} -I_2 & O \\ \hline O & -I_2 \end{array} \right) = -I_4
        \]

        \medskip

        (b):

        For \(A,B,C,C \in \mathcal{M}_2(\mathbb{F})\), let \(X = \left( \begin{array}{c|c} A & B \\ \hline C & D \end{array} \right) \in \mathcal{M} _4 (\mathbb{F})\) and suppose that \(XJ = JX\). Then we have that
        \[
            \left( \begin{array}{c|c} A & B \\ \hline C & D \end{array} \right) \left( \begin{array}{c|c} O & -I_2 \\ \hline I_2 & O \end{array} \right) = \left( \begin{array}{c|c} O & -I_2 \\ \hline I_2 & O \end{array} \right) \left( \begin{array}{c|c} A & B \\ \hline C & D \end{array} \right)
        \]
        \[
            \implies \left( \begin{array}{c|c} AO + BI_2 & A(-I_2) + BO \\ \hline CO + DI_2 & C(-I_2) \end{array} \right) = \left( \begin{array}{c|c} OA + -I_2 C & OB - I_2 D \\ \hline I_2 A + OC & I_2 B + OD \end{array} \right)
        \]
        \[
            \implies \left( \begin{array}{c|c} B & -A \\ \hline D & -C \end{array} \right) = \left( \begin{array}{c|c} -C & -D \\ \hline A & B \end{array} \right)
        \]
        \[
            \implies B = -C \text{ and } A = -D
        \]
        Therefore all \(X\) that satisfy this equation are of the form
        \[
            X = \left( \begin{array}{c|c} P & -Q \\ \hline -Q & P \end{array} \right), \text{ where } P,Q \in M_2(\mathbb{F}).
        \]

        \medskip

        (c):

        Notice that \(X \in \mathfrak{sp}_4\) if and only if \(X\) can be decomposed into a block matrix such that
        \[
            X = \left( \begin{array}{c|c} P & Q \\ \hline -Q & -P \end{array} \right)
        \]
        for some matrices \(P,Q \in M_2(\mathbb{F})\).

        We know that \(0 \in \mathfrak{sp}_4\) because as a \(4\times 4\) matrix, \(0 = \left( \begin{array}{c|c} O & O \\ \hline O & O \end{array} \right)\), which is clearly satisfies the condition outlined above.

        Now, let \(c \in \mathbb{F}\) and \(X,Y \in \mathfrak{sp}_4\). Then for some \(2\times 2\) matrices \(P,Q,R,S\),
        \[
            X = \left( \begin{array}{c|c} P & Q \\ \hline -Q & -P \end{array} \right) \text{ and } Y = \left( \begin{array}{c|c} R & S \\ \hline -S & -R \end{array} \right)
        \]
        It follows that
        \[
            cX + Y = c\left( \begin{array}{c|c} P & Q \\ \hline -Q & -P \end{array} \right) + \left( \begin{array}{c|c} R & S \\ \hline -S & -R \end{array} \right) = \left( \begin{array}{c|c} cP + R & cQ + S \\ \hline -cQ -S & -cP-R \end{array} \right)
        \]
        so \(cX + Y \in \mathfrak{sp}_4\). Thus we can conclude that \(\mathfrak{sp}_4\) is indeed a subspace.

        \medskip

        (d):

        For \(1 \leq i,j \leq 2\), let \(e_{ij}\) be the \((i,j)\)th standard basis vector for \(M_2(\mathbb{F})\). We claim that a basis for \(\mathfrak{sp}_4\) is given by
        \[
            \beta = \left\{ X \in \mathcal{M}_4(\mathbb{F}) : X = \left( \begin{array}{c|c} e_{ij} & O \\ \hline O & -e_{ij} \end{array} \right) \text{ or } X = \left( \begin{array}{c|c} O & e_{ij} \\ \hline -e_{ij} & O \end{array} \right)\right\}.
        \]
        Let \(A \in \mathfrak{sp}_4\). Then for matrices \(P,Q \in \mathcal{M} _2(\mathbb{R})\),
        \[
            A = \left( \begin{array}{c|c} P & Q \\ \hline -Q & -P \end{array} \right)
        \]
        We can rewrite \(P,Q\) as linear combinations of basis vectors:
        \[
            A = \left( \begin{array}{c|c} \sum\limits_{i,j=1}^{2} p_{ij} e_{ij} & \sum\limits_{i,j=1}^{2} q_{ij} e_{ij} \\ \hline -\sum\limits_{i,j=1}^{2} p_{ij} e_{ij} & -\sum\limits_{i,j=1}^{2} q_{ij} e_{ij}\end{array}\right) = \sum_{i,j=1}^{2} p_{ij} \left( \begin{array}{c|c} e_{ij} & O \\ \hline O & -e_{ij} \end{array} \right) + \sum_{i,j=1}^{2} q_{ij} \left( \begin{array}{c|c} O & e_{ij} \\ \hline -e_{ij} & O \end{array} \right)
        \]
        Thus \(\beta\) spans \(\mathfrak{sp}_4\).

        Now, suppose that
        \[
            = \sum_{i,j=1}^{2} p_{ij} \left( \begin{array}{c|c} e_{ij} & O \\ \hline O & -e_{ij} \end{array} \right) + \sum_{i,j=1}^{2} q_{ij} \left( \begin{array}{c|c} O & e_{ij} \\ \hline -e_{ij} & O \end{array} \right) = 0
        \]
        Then it must be true that
        \[
            \sum_{i,j=1}^{2} p_{ij} e_{ij} = \sum_{i,j=1}^{2} q_{ij} e_{ij} = 0
        \]
        Since the standard basis vectors are linearly independent, it follows that all \(p_{ij} = q_{ij} = 0\), thus proving that \(\beta\) is a basis for \(\mathfrak{sp}_4\).

        For every of the four standard basis vector \(e_{ij}\) of \(\mathcal{M} _2(\mathbb{F})\), there are two vectors in \(\beta\) that correspond to it, so \(\dim V = |\beta | = 2\cdot 4 = 8\).
    \end{question}
    \newpage
    \begin{question}{11}
        Determine if the statements below are true or false. If true, give a proof. If false, explain why, and/or provide a counterexample.
        
        \begin{enumerate}[label=(\alph*)]
            \item Let \( V \) be a finite dimensional vector space over \( \mathbb{F} \). If \( I \subseteq V \) is a linearly independent set so that for any \( x \in V \setminus I \), the set \( I \cup \{ x \} \) is linearly dependent, then \( I \) is a basis for \( V \).
            \item Let \( V \) be a finite dimensional vector space over \( \mathbb{F} \). If \( S \subseteq V \) is a spanning set so that \( |S| = \dim V \), then \( S \) is a basis for \( V \).
            \item Let \( V \) be a finite dimensional vector space over \( \mathbb{F} \). If \( W \subseteq V \) a subspace, then there exists a unique subspace \( U \subseteq V \) so that \( V = W \oplus U \).
        \end{enumerate}
        \tcblower
        \ 

        (a):

        Let \(V\) be a finite dimensional vector space over \(\mathbb{F}\). Suppose \(I \subseteq V\) is linearly independent and that adding any vector in \(V\setminus I\) will result in the set no longer being linearly independent, and note that the same also applies when choosing a vector that is in \(I\). Then for any \(x \in V\), we have that for some vectors \(v_1, ..., v_n \in I\),
        \[
            cx + \sum_{i=1}^{n} c_i v_i = 0
        \]
        for \(c, c_i \in \mathbb{F}\) not all zero. We make the important note that it is necessary for \(c \neq 0\), because if not, then
        \[
            \sum_{i=1}^{n} c_i v_i = 0
        \]
        which implies that all coefficients are zero by independence, which contradicts our claim that not all coefficients were zero. It follows that \(c\) has an inverse \(c^{-1}\) and
        \[
            x = \sum_{i=1}^{n} -c^{-1}c_i v_i.
        \]
        Since every \(x \in V\) is a linear combination of vectors in \(I\), it follows that \(I\) is indeed a basis for \(V\).

        \medskip

        (b):

        Suppose for contradiction that \(S\) is not a basis for \(V\). Then \(S\) is not linearly independent, that is, for some \(s \in S\), \(c_i \in \mathbb{F}\), \(s_i \in S\setminus \{ s \}\),
        \[
            s = \sum_{i=1}^{n} c_i s_i
        \]
        This means that \(S\setminus \{ s \}\) is also a spanning set. But \(|S\setminus \{ s \} | < \dim V\), which is a contradiction by Question 5. Thus \(S\) is a basis for \(V\).

        \medskip

        (c):

        Suppose that \(W\) is a subspace of \(V\). Then it has a basis \(\beta _W\), which is linearly independent. By Question 5, we can extend \(\beta _W\) to a basis for \(V\) by combining it with some linearly independent set \(\alpha\). We claim that our desired subspace is \(U = \mathrm{span} \alpha\). Since \(\beta _W\) is disjoint from \(\alpha\), by Question 9, \(V = W \oplus U\).

        Now, suppose that \(U_1, U_2\) are subspaces such that \(V = W \oplus U_1 = W\oplus U_2\).

        Let \(u \in U_1\). It follows that \(u \in V = W \oplus U_2\), so \(u = w + v\), for some \(w \in W\) and \(v \in U_2\).
    \end{question}
\end{document}