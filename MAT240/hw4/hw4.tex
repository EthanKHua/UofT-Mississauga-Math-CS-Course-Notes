\documentclass{eh-homework}

\begin{document}
\begin{question}{1}
    Use row operations on the matrix $A = \begin{pmatrix}
    1 & 0 & 2 & 1 \\
    1 & 2 & 3 & 2 \\
    1 & -2 & 6 & 3 \\
    2 & 4 & -6 & -2
    \end{pmatrix}$ to obtain an upper triangular matrix, then use Theorem 59 to find $\det A$. (You will get no credit for using a row/column expansion.)

    \bigskip

    We have
    \begin{align*}
        \det A &= \det \begin{pmatrix}
            1 & 0 & 2 & 1 \\
            1 & 2 & 3 & 2 \\
            1 & -2 & 6 & 3 \\
            2 & 4 & -6 & -2
            \end{pmatrix} \\
            &= \det \begin{pmatrix}
                1 & 0 & 2 &  1 \\
                0 & 2 & 1 &  1 \\
                0 & -2 & 4 &  2 \\
                0 & 4 & -10 &  -4 \\
            \end{pmatrix} \\
            &= \det \begin{pmatrix}
                1 & 0 & 2 &  1 \\
                0 & 2 & 1 &  1 \\
                0 & 0 & 5 &  3 \\
                0 & 0 & -12 &  -6 \\
            \end{pmatrix} \\
            &= -6\det \begin{pmatrix}
                1 & 0 & 2 &  1 \\
                0 & 2 & 1 &  1 \\
                0 & 0 & 5 &  3 \\
                0 & 0 & 2 &  1 \\
            \end{pmatrix} \\
            &= 6\det \begin{pmatrix}
                1 & 0 & 2 &  1 \\
                0 & 2 & 1 &  1 \\
                0 & 0 & 2 &  1 \\
                0 & 0 & 5 &  3 \\
            \end{pmatrix} \\
            &= 6\det \begin{pmatrix}
                1 & 0 & 2 &  1 \\
                0 & 2 & 1 &  1 \\
                0 & 0 & 2 &  1 \\
                0 & 0 & 0 &  \frac{1}{2} \\
            \end{pmatrix} \\
            &= 6(1)(2)(2)\left(\frac{1}{2}\right) \\
            &= 12
    \end{align*}
    \end{question}
    
    \begin{question}{2}
    Let $T = T_A : \mathbb{Q}^5 \to \mathbb{Q}^5$ where $A = \begin{pmatrix}
    1 & 0 & 1 & -2 & 0 \\
    3 & 0 & 1 & 0 & -2 \\
    2 & 0 & 0 & 2 & -2 \\
    2 & 0 & 0 & 1 & -2 \\
    2 & 0 & 1 & -2 & -1
    \end{pmatrix}$.
    
    \begin{enumerate}[label=(\alph*)]
        \item Find $C_T$ and the eigenvalues of $T$.
        
        We have
        \begin{align*}
            C_T(\lambda) &= \det (\lambda I - T) \\
            &= \det \begin{pmatrix}
                \lambda - 1 & 0 & -1 & 2 &  0 \\
                -3 & \lambda & -1 & 0 &  2 \\
                -2 & 0 & \lambda & -2 &  2 \\
                -2 & 0 & 0 & \lambda - 1 &  2 \\
                -2 & 0 & -1 & 2 &  \lambda + 1 \\
            \end{pmatrix} \\
            &= -\lambda \det \begin{pmatrix}
                \lambda - 1 & -1 & 2 &  0 \\
                -2 & \lambda & -2 &  2 \\
                -2 & 0 & \lambda - 1 &  2 \\
                -2 & -1 & 2 &  \lambda + 1 \\
            \end{pmatrix} \\
            &= -\lambda \det \begin{pmatrix}
                \lambda + 1 & 0 & 0 &  -\lambda - 1 \\
                -2 & \lambda & -2 &  2 \\
                -2 & 0 & \lambda - 1 &  2 \\
                -2 & -1 & 2 &  \lambda + 1 \\
            \end{pmatrix} \\
            &= -\lambda \left( (\lambda + 1)\det \begin{pmatrix}
                \lambda & -2 &  2 \\
                0 & \lambda-1 &  2 \\
                -1 & 2 &  \lambda+1 \\
            \end{pmatrix} +(\lambda+1)\det \begin{pmatrix}
                -2 & \lambda &  -2 \\
                -2 & 0 &  \lambda-1 \\
                -2 & -1 &  2 \\
            \end{pmatrix} \right) \\
            &= \lambda(\lambda + 1)\left( -(\lambda - 1)(\lambda(\lambda+1) + 2) + 2(2\lambda - 2) + -2(2\lambda - 2) + (\lambda - 1)(2 + 2\lambda) \right) \\
            &= \lambda(\lambda+1)\left( (\lambda - 1)(2 - \lambda - \lambda^2 - 2 + 2\lambda\right) \\
            &=\lambda(\lambda+1)(\lambda - 1)(\lambda - \lambda^2) \\
            &= - \lambda^2 (\lambda - 1)^2 (\lambda + 1)
        \end{align*}

        The eigenvalues are the roots of \(C_T\), which are \(\lambda = 0, 1, -1\).

        \item For each eigenvalue, find a basis for the corresponding eigenspace.
        
        For \(\lambda = 0\), we solve the equation \(Ax = 0\) via row reduction:
        \begin{align*}
            \left( \begin{array}{@{}ccccc|c@{}}
                1 & 0 & 1 & -2 & 0 & 0\\
                3 & 0 & 1 & 0 & -2 & 0\\
                2 & 0 & 0 & 2 & -2 & 0\\
                2 & 0 & 0 & 1 & -2 & 0\\
                2 & 0 & 1 & -2 & -1 & 0 \\
            \end{array} \right) &\to
            \left( \begin{array}{@{}ccccc|c@{}}
                1 & 0 & 1 & -2 & 0 & 0\\
                0 & 0 & -2 & 6 & -2 & 0\\
                0 & 0 & -2 & 6 & -2 & 0\\
                0 & 0 & -2 & 5 & -2 & 0\\
                0 & 0 & -1 & 2 & -1 & 0 \\
            \end{array} \right) \to 
            \left( \begin{array}{@{}ccccc|c@{}}
                1 & 0 & 0 & 1 & 0 & 0\\
                0 & 0 & 1 & 3 & 1 & 0\\
                0 & 0 & 0 & 0 & 0 & 0\\
                0 & 0 & 0 & -1 & 0 & 0\\
                0 & 0 & 0 & -1 & 0 & 0 \\
            \end{array} \right) \\
            &\to \left( \begin{array}{@{}ccccc|c@{}}
                1 & 0 & 0 & 0 & 0 & 0\\
                0 & 0 & 1 & 0 & 1 & 0\\
                0 & 0 & 0 & 1 & 0 & 0\\
                0 & 0 & 0 & 0 & 0 & 0\\
                0 & 0 & 0 & 0 & 0 & 0 \\
            \end{array} \right)
        \end{align*}
        We get that \(x_1 = 0, x_4 = 0, x_3 + x_5 = 0\). We parametrize \(x_2 = t, x_3 = s\) and get
        \[
            x = \begin{pmatrix}
                 0 \\
                 t \\
                 s \\
                 0 \\
                 -s \\
            \end{pmatrix} =
            te_2 + s\begin{pmatrix}
                 0 \\
                 0 \\
                 1 \\
                 0 \\
                 -1 \\
            \end{pmatrix}
        \]
        Thus a basis for \(E_0(T)\) is \( \left\{ e_2, \begin{pmatrix}
             0 \\
             0 \\
             1 \\
             0 \\
             -1 \\
        \end{pmatrix}\right\} \).

        \medskip

        For \(\lambda = 1\), to solve \((A - I)x = 0\), we get the system
        \begin{align*}
            \left( \begin{array}{@{}ccccc|c@{}}
                0 & 0 & 1 & -2 & 0 & 0\\
                3 & -1 & 1 & 0 & -2 & 0\\
                2 & 0 & -1 & 2 & -2 & 0\\
                2 & 0 & 0 & 0 & -2 & 0\\
                2 & 0 & 1 & -2 & -2 & 0 \\
            \end{array} \right) &\to
            \left( \begin{array}{@{}ccccc|c@{}}
                0 & 0 & 1 & -2 & 0 & 0\\
                1 & -1 & 2 & -2 & 0 & 0\\
                2 & 0 & -1 & 2 & -2 & 0\\
                0 & 0 & 1 & -2 & 0 & 0\\
                0 & 0 & 2 & -4 & 0 & 0 \\
            \end{array} \right) \\
            &\to 
            \left( \begin{array}{@{}ccccc|c@{}}
                0 & 0 & 1 & -2 & 0 & 0\\
                1 & -1 & 0 & 2 & 0 & 0\\
                0 & 2 & 0 & -4 & -2 & 0\\
                0 & 0 & 0 & 0 & 0 & 0\\
                0 & 0 & 0 & 0 & 0 & 0 \\
            \end{array} \right) \\
            &\to 
            \left( \begin{array}{@{}ccccc|c@{}}
                0 & 0 & 1 & -2 & 0 & 0\\
                1 & 0 & 0 & 0 & -1 & 0\\
                0 & 1 & 0 & -2 & -1 & 0\\
                0 & 0 & 0 & 0 & 0 & 0\\
                0 & 0 & 0 & 0 & 0 & 0 \\
            \end{array} \right)
        \end{align*}
        We parametrize \(x_4 = t, x_5 = s\) to get
        \[
            x = \begin{pmatrix}
                s  \\
                2t + s  \\
                2t  \\
                t  \\
                s  \\
            \end{pmatrix} = 
            s \begin{pmatrix}
                 1 \\
                 1 \\
                 0 \\
                 0 \\
                 1 \\
            \end{pmatrix} +
            t \begin{pmatrix}
                 0 \\
                 2 \\
                 2 \\
                 1 \\
                 0 \\
            \end{pmatrix}
        \]
        Thus a basis for \(E_1(T)\) is \(\left\{ \begin{pmatrix}
            1 \\
            1 \\
            0 \\
            0 \\
            1 \\
       \end{pmatrix},\begin{pmatrix}
            0 \\
            2 \\
            2 \\
            1 \\
            0 \\
        \end{pmatrix} \right\} \).

        \medskip

        For \(\lambda = -1\), we solve \((A + I)x = 0\):
        \begin{align*}
            \left( \begin{array}{@{}ccccc|c@{}}
                2 & 0 & 1 & -2 & 0 & 0\\
                3 & 1 & 1 & 0 & -2 & 0\\
                2 & 0 & 1 & 2 & -2 & 0\\
                2 & 0 & 0 & 2 & -2 & 0\\
                2 & 0 & 1 & -2 & 0 & 0 \\
            \end{array} \right) &\to 
            \left( \begin{array}{@{}ccccc|c@{}}
                2 & 0 & 1 & -2 & 0 & 0\\
                1 & 1 & 0 & 2 & -2 & 0\\
                0 & 0 & 0 & 4 & -2 & 0\\
                0 & 0 & -1 & 4 & -2 & 0\\
                0 & 0 & 0 & 0 & 0 & 0 \\
            \end{array} \right) \\
            &\to \left( \begin{array}{@{}ccccc|c@{}}
                1 & 1 & 0 & 2 & -2 & 0\\
                2 & 0 & 1 & -2 & 0 & 0\\
                0 & 0 & -1 & 4 & -2 & 0\\
                0 & 0 & 0 & 4 & -2 & 0\\
                0 & 0 & 0 & 0 & 0 & 0 \\
            \end{array} \right) \\
            &\to \left( \begin{array}{@{}ccccc|c@{}}
                1 & 1 & 0 & 2 & -2 & 0\\
                0 & -2 & 1 & -4 & 4 & 0\\
                0 & 0 & -1 & 4 & -2 & 0\\
                0 & 0 & 0 & 4 & -2 & 0\\
                0 & 0 & 0 & 0 & 0 & 0 \\
            \end{array} \right) \\
            &\to \left( \begin{array}{@{}ccccc|c@{}}
                1 & 1 & 0 & 2 & -2 & 0\\
                0 & -2 & 0 & 0 & 2 & 0\\
                0 & 0 & -1 & 4 & -2 & 0\\
                0 & 0 & 0 & 4 & -2 & 0\\
                0 & 0 & 0 & 0 & 0 & 0 \\
            \end{array} \right) \\
            &\to \left( \begin{array}{@{}ccccc|c@{}}
                1 & 1 & 0 & 0 & -1 & 0\\
                0 & -2 & 0 & 0 & 2 & 0\\
                0 & 0 & -1 & 0 & 0 & 0\\
                0 & 0 & 0 & 4 & -2 & 0\\
                0 & 0 & 0 & 0 & 0 & 0 \\
            \end{array} \right) \\
            &\to \left( \begin{array}{@{}ccccc|c@{}}
                1 & 1 & 0 & 0 & -1 & 0\\
                0 & -1 & 0 & 0 & 1 & 0\\
                0 & 0 & -1 & 0 & 0 & 0\\
                0 & 0 & 0 & 1 & -\frac{1}{2} & 0\\
                0 & 0 & 0 & 0 & 0 & 0 \\
            \end{array} \right)\\
            &\to \left( \begin{array}{@{}ccccc|c@{}}
                1 & 0 & 0 & 0 & 0 & 0\\
                0 & -1 & 0 & 0 & 1 & 0\\
                0 & 0 & -1 & 0 & 0 & 0\\
                0 & 0 & 0 & 1 & -\frac{1}{2} & 0\\
                0 & 0 & 0 & 0 & 0 & 0 \\
            \end{array} \right)\\
        \end{align*}
        Let \(x_5 = t\). The general solution is
        \[
            x = t\begin{pmatrix}
                 0 \\
                 2 \\
                 0 \\
                 1 \\
                 0 \\
            \end{pmatrix}
        \]
        So a basis for \(E_{-1}(T)\) is \(\left\{ \begin{pmatrix}
            0 \\
            2 \\
            0 \\
            1 \\
            0 \\
       \end{pmatrix} \right\}\).

        \item Determine if $T$ is diagonalizable, and if so, find a basis $\beta$ so that $[T]_\beta$ is diagonal.
        
        Since the dimension of each eigenspace matches the algebraic multiplicity of each corresponding eigenvalue, \(T\) is diagonalizable and the basis \(\beta\) that makes \([T]_\beta\) diagonal is exactly the basis consisting of the basis vectors of each eigenspace. In particular,
        \[
            \beta = \left\{ e_2, \begin{pmatrix}
                0 \\
                0 \\
                1 \\
                0 \\
                -1 \\
           \end{pmatrix},\begin{pmatrix}
            1 \\
            1 \\
            0 \\
            0 \\
            1 \\
       \end{pmatrix},\begin{pmatrix}
            0 \\
            2 \\
            2 \\
            1 \\
            0 \\
        \end{pmatrix},\begin{pmatrix}
            0 \\
            2 \\
            0 \\
            1 \\
            0 \\
       \end{pmatrix} \right\}.
        \]
    \end{enumerate}
    \end{question}
    
    \begin{question}{3}
    \begin{enumerate}[label=(\alph*)]
        \item Read the proof of Theorem 58 from the additional file in the Week 10 Readings on the course page.
        \item Prove Part 1 of Theorem 59 using a strategy similar to the proof of Theorem 58. (You cannot use other parts of Theorem 59 in this proof.)
    \end{enumerate}
    Let \(A \in M_n(\mathbb{F})\) with \(n \geq 2\). If \(A\) has a row of 0's, then \(\det A = 0\).
    \tcblower
    Write \(A = \begin{pmatrix}
         r_1 \\
         \vdots \\
         r_n \\
    \end{pmatrix}\), where \(r_j\) represents the rows of \(A\). Suppose that \(r_i = \vec{0}\). If \(i = 1\), the result is immediate by cofactor expansion. Otherwise, if \(i > 1\), we do induction on \(n\).

    Let \(n = 2\). The only possibility is \(i = 2\), so denote
    \[
        A = \begin{pmatrix}
            a &  b \\
            0 &  0 \\
        \end{pmatrix}.
    \]
    From here, it is easy to see that \(\det A = 0\).

    Now, suppose that this is true for some \(n\). We will show that it is true for \(n + 1\). Define \(\tilde{r}_{j,k}\) to be the row obtained by deleting the \(k\)th entry of \(r_j\). Using cofactor expansion along the first row, we have
    \[
        \det A = \sum_{k=1}^{n+1} A_{1k}\det \tilde{A}_{1k}
    \]
    Observe that \(\tilde{A}_{1k}\) are \(n\times n\) matrices, and since the \(i\)th row was 0 in the original matrix (and \(i > 1\)), the \(i - 1\)th row in \(\tilde{A}_{1k}\) is 0, so by the induction hypothesis \(\det \tilde{A}_{1k} = 0\), thus \(\det A = 0\) and we are done.

    \end{question}
    
    \begin{question}{4}
    Assume that Parts 1 and 2 of Theorem 59 have been proved. You cannot use Parts 4 through 7 of Theorem 59 in the following problem.
    
    \begin{enumerate}[label=(\alph*)]
        \item Prove Part 3 using induction on $n$. (Check $n = 1, 2$ by hand, then in the inductive step assume $n+1 \ge 3$.)
        \item Prove Part 4 using row-swapping matrices and properties of determinants.
    \end{enumerate}
    \tcblower
    \ 

    (a):

    We prove Part 3 using induction on \(n\).

    Let \(n = 1\). The statement is vacuously true, as \(A\) cannot have 2 identical rows.

    Let \(n = 2\). Then it must be true that
    \[
        A = \begin{pmatrix}
            a &  b \\
            a &  b \\
        \end{pmatrix}, \text{ for } a,b \in \mathbb{F}.
    \]
    Then \(\det A = ab - ab = 0\) as expected.

    Now, suppose that this statement is true for some \(n \in \mathbb{N}\), where \(n > 1\). We will show it also holds for \(n + 1\). Let \(r_i,r_j\) be the identical rows. Since \(n + 1 > 2\), we are guaranteed to have one other row \(r_k\) that is not \(r_i\) or \(r_j\). We perform a row \(k\) expansion of \(\det A\) and see that
    \[
        \det A = \sum_{l=1}^{n + 1} A_{kl} \det \tilde{A}_{kl}
    \]
    Notice that \(\tilde{A}_{kl}\) is a \(n\times n\) matrix, and contain both \(r_i\) and \(r_j\) with the \(l\)th entry deleted. But these rows are still identical because the same entry got deleted. By the induction hypothesis,
    \[
        \det A = \sum_{l=1}^{n + 1} A_{kl} 0 = 0
    \]
    which was what we wanted.

    \bigskip

    (b):

    Suppose that \(B\) is obtained from \(A\) by swapping row \(i\) and row \(j\). Denote these rows as \(r_i,r_j\) respectively. Using linearity in one row of the determinant, and the previous result we proved,
    \[
        0 = \det \begin{pmatrix}
             r_1 \\
             \vdots \\
             r_i + r_j\\
             \vdots \\
             r_i + r_j \\
             \vdots \\
             r_n
        \end{pmatrix} =
        \det \begin{pmatrix}
            r_1 \\
            \vdots \\
            r_i\\
            \vdots \\
            r_i + r_j \\
            \vdots \\
            r_n
       \end{pmatrix} +
       \det \begin{pmatrix}
            r_1 \\
            \vdots \\
            r_j\\
            \vdots \\
            r_i + r_j \\
            \vdots \\
            r_n
        \end{pmatrix}
    \]
    \[
        \implies 0 = \det \begin{pmatrix}
            r_1 \\
            \vdots \\
            r_i\\
            \vdots \\
            r_j \\
            \vdots \\
            r_n
       \end{pmatrix}+
       \det \begin{pmatrix}
            r_1 \\
            \vdots \\
            r_i\\
            \vdots \\
            r_i \\
            \vdots \\
            r_n
        \end{pmatrix}+
        \det \begin{pmatrix}
            r_1 \\
            \vdots \\
            r_j\\
            \vdots \\
            r_i \\
            \vdots \\
            r_n
       \end{pmatrix}+
       \det \begin{pmatrix}
            r_1 \\
            \vdots \\
            r_j\\
            \vdots \\
            r_j \\
            \vdots \\
            r_n
        \end{pmatrix}
    \]
    \[
        \implies 0 = \det A + 0 + \det B + 0
    \]
    \[
        \implies \det B = -\det A
    \]
    as needed.
    \end{question}
    
    \begin{question}{5}
    Prove that if $U \in M_{n \times n}(F)$ is upper triangular, then $\det U = \prod_{i=1}^n U_{ii}$.
    \tcblower
    We proceed using induction on \(n\). If \(n=1\), the result is immediate. Suppose that the statement holds for some \(n \in \mathbb{N}\). We will show the same is the case for \(n + 1\).

    Let \(U \in M_{n+1} (\mathbb{F})\) be upper triangular. We have
    \[
        \det U = \sum_{j=1}^{n+1} U_{1j} \det \tilde{U}_{1j}.
    \]
    For \(j \neq 1\), notice that the entries of the first column of \(\tilde{U}_{1j}\) are 0, as \((\tilde{U}_{1j})_{i1} = U_{(i+1)1} = 0\). Thus
    \[
        \det \tilde{U}_{1j} = \det \tilde{U}_{1j}^t = 0
    \]
    as the transpose has a row of 0's. The cofacter expansion of \(\det U\) reduces to
    \[
        \det U = U_{11} \det \tilde{U}_{11}
    \]
    but \(\tilde{U}_{11} \in M_n(\mathbb{F})\) is upper triangular, so
    \[
        \det U = U_{11} \prod _{i=1}^n \tilde{U}_{ii} = U_{11} \prod _{i=2}^{n+1} U_{ii} = \prod _{i=1}^{n+1} U_{ii}. 
    \]
    which completes the proof.
    \end{question}
    
    \begin{question}{6}
    Let $V$ be a vector space over $F$, and $T : V \to V$ a linear map. If $W \subseteq V$ is a $T$-invariant subspace, then we can restrict \(T\) to \(W\), to obtain a map \(T_W : W \to W\). We call \(T_W\) the restriction map.
    
    \begin{enumerate}[label=(\alph*)]
        \item Let \(\beta _W\) be a basis for \(W\). In HW\#3 we proved that if \(\beta = \beta _W \beta _1\) is an extension of \(\beta_W\) to a basis for \(V\), then \([T]_\beta = \left( \begin{array}{@{}c|c@{}}
            A & B \\
            \hline
            O & C \\
        \end{array}\right)\). Prove that \(A = [T_W]_{\beta_W}\).

        \item Let \(M = \left(\begin{array}{@{}c|c@{}}
            A & B \\
            \hline
            O & C \\
        \end{array}\right)\). Prove that \(\det M = \det A \det C\).
    \end{enumerate}
    \tcblower
    \ 

    (a):

    Denote \(\beta = \{ w_1, ..., w_n \}\). Then the \(j\)th column of \(\left( \begin{array}{@{}c@{}}
        A \\
        \hline
        O \\
    \end{array} \right)\) is \([T(w_j)]_\beta\).
    \end{question}
    
    \begin{question}{7}
    Deduce from Question 6 that if $W$ is a $T$-invariant subspace, then $C_{T_W}$ divides $C_T$.
    \end{question}
    
    \begin{question}{8}
    Let $V$ be a finite-dimensional vector space over a field $F$, and $W_1, W_2 \subseteq V$ such that $V = W_1 \oplus W_2$. Define the projection maps $P_i : V \to V$ by $P(x) = x_i$ where $x = x_1 + x_2$ with $x_1 \in W_1$ and $x_2 \in W_2$.
    
    \begin{enumerate}
        \item Prove that $P_i$ is linear.
        \item Prove that $P_i^2 = P_i$.
        \item Prove that each $W_j$ is $P_i$-invariant.
        \item Determine if $P_i$ is diagonalizable and justify your answer.
    \end{enumerate}
    \end{question}
    
    \begin{question}{9}
    Define the direct sum for more than two subspaces. Let $W_1, \ldots, W_k \subseteq V$ be subspaces such that $V = W_1 \oplus \cdots \oplus W_k$.
    
    \begin{enumerate}
        \item Prove that every basis $\beta$ for $V$ gives a direct sum decomposition $V = W_1 \oplus \cdots \oplus W_n$ where $\dim W_i = 1$.
        \item Prove the converse: If $V = W_1 \oplus \cdots \oplus W_n$ with $\dim W_i = 1$, then choosing non-zero $w_i \in W_i$ forms a basis for $V$.
        \item Let $T : V \to V$ be linear. Show that $[T]_\beta$ is block diagonal.
    \end{enumerate}
    \end{question}
    
    \begin{question}{10}
    Let $W_1, \ldots, W_k \subseteq V$ with bases $\beta_1, \ldots, \beta_k$. Prove that $V = W_1 \oplus \cdots \oplus W_k$ if and only if $\beta = \beta_1 \cup \cdots \cup \beta_k$ is a basis for $V$.
    \end{question}
    
    \begin{question}{11}
    Determine whether the following statements are true or false. Justify your answers.
    
    \begin{enumerate}
        \item If $V = W_1 \oplus W_2$ and $T_{W_1}, T_{W_2}$ are diagonalizable, then $T$ is diagonalizable.
        \item If $W_i \cap W_j = \{0\}$ for $i \neq j$ and $V = W_1 + W_2 + W_3$, then $V = W_1 \oplus W_2 \oplus W_3$.
        \item If $\dim V = 7$, $\dim N(T) = 3$, and $\operatorname{rank}(T - I) = 4$, then $T$ is diagonalizable.
    \end{enumerate}
    \end{question}
\end{document}