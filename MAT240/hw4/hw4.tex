\documentclass{eh-homework}

\begin{document}
\begin{question}{1}
    Use row operations on the matrix $A = \begin{pmatrix}
    1 & 0 & 2 & 1 \\
    1 & 2 & 3 & 2 \\
    1 & -2 & 6 & 3 \\
    2 & 4 & -6 & -2
    \end{pmatrix}$ to obtain an upper triangular matrix, then use Theorem 59 to find $\det A$. (You will get no credit for using a row/column expansion.)

    \bigskip

    We have
    \begin{align*}
        \det A &= \det \begin{pmatrix}
            1 & 0 & 2 & 1 \\
            1 & 2 & 3 & 2 \\
            1 & -2 & 6 & 3 \\
            2 & 4 & -6 & -2
            \end{pmatrix} \\
            &= \det \begin{pmatrix}
                1 & 0 & 2 &  1 \\
                0 & 2 & 1 &  1 \\
                0 & -2 & 4 &  2 \\
                0 & 4 & -10 &  -4 \\
            \end{pmatrix} \\
            &= \det \begin{pmatrix}
                1 & 0 & 2 &  1 \\
                0 & 2 & 1 &  1 \\
                0 & 0 & 5 &  3 \\
                0 & 0 & -12 &  -6 \\
            \end{pmatrix} \\
            &= -6\det \begin{pmatrix}
                1 & 0 & 2 &  1 \\
                0 & 2 & 1 &  1 \\
                0 & 0 & 5 &  3 \\
                0 & 0 & 2 &  1 \\
            \end{pmatrix} \\
            &= 6\det \begin{pmatrix}
                1 & 0 & 2 &  1 \\
                0 & 2 & 1 &  1 \\
                0 & 0 & 2 &  1 \\
                0 & 0 & 5 &  3 \\
            \end{pmatrix} \\
            &= 6\det \begin{pmatrix}
                1 & 0 & 2 &  1 \\
                0 & 2 & 1 &  1 \\
                0 & 0 & 2 &  1 \\
                0 & 0 & 0 &  \frac{1}{2} \\
            \end{pmatrix} \\
            &= 6(1)(2)(2)\left(\frac{1}{2}\right) \\
            &= 12
    \end{align*}
    \end{question}
    \newpage
    \begin{question}{2}
    Let $T = T_A : \mathbb{Q}^5 \to \mathbb{Q}^5$ where $A = \begin{pmatrix}
    1 & 0 & 1 & -2 & 0 \\
    3 & 0 & 1 & 0 & -2 \\
    2 & 0 & 0 & 2 & -2 \\
    2 & 0 & 0 & 1 & -2 \\
    2 & 0 & 1 & -2 & -1
    \end{pmatrix}$.
    
    \begin{enumerate}[label=(\alph*)]
        \item Find $C_T$ and the eigenvalues of $T$.
        
        We have
        \begin{align*}
            C_T(\lambda) &= \det (\lambda I - T) \\
            &= \det \begin{pmatrix}
                \lambda - 1 & 0 & -1 & 2 &  0 \\
                -3 & \lambda & -1 & 0 &  2 \\
                -2 & 0 & \lambda & -2 &  2 \\
                -2 & 0 & 0 & \lambda - 1 &  2 \\
                -2 & 0 & -1 & 2 &  \lambda + 1 \\
            \end{pmatrix} \\
            &= -\lambda \det \begin{pmatrix}
                \lambda - 1 & -1 & 2 &  0 \\
                -2 & \lambda & -2 &  2 \\
                -2 & 0 & \lambda - 1 &  2 \\
                -2 & -1 & 2 &  \lambda + 1 \\
            \end{pmatrix} \\
            &= -\lambda \det \begin{pmatrix}
                \lambda + 1 & 0 & 0 &  -\lambda - 1 \\
                -2 & \lambda & -2 &  2 \\
                -2 & 0 & \lambda - 1 &  2 \\
                -2 & -1 & 2 &  \lambda + 1 \\
            \end{pmatrix} \\
            &= -\lambda \left( (\lambda + 1)\det \begin{pmatrix}
                \lambda & -2 &  2 \\
                0 & \lambda-1 &  2 \\
                -1 & 2 &  \lambda+1 \\
            \end{pmatrix} +(\lambda+1)\det \begin{pmatrix}
                -2 & \lambda &  -2 \\
                -2 & 0 &  \lambda-1 \\
                -2 & -1 &  2 \\
            \end{pmatrix} \right) \\
            &= \lambda(\lambda + 1)\left( -(\lambda - 1)(\lambda(\lambda+1) + 2) + 2(2\lambda - 2) + -2(2\lambda - 2) + (\lambda - 1)(2 + 2\lambda) \right) \\
            &= \lambda(\lambda+1)\left( (\lambda - 1)(2 - \lambda - \lambda^2 - 2 + 2\lambda\right) \\
            &=\lambda(\lambda+1)(\lambda - 1)(\lambda - \lambda^2) \\
            &= - \lambda^2 (\lambda - 1)^2 (\lambda + 1)
        \end{align*}

        The eigenvalues are the roots of \(C_T\), which are \(\lambda = 0, 1, -1\).

        \item For each eigenvalue, find a basis for the corresponding eigenspace.
        
        For \(\lambda = 0\), we solve the equation \(Ax = 0\) via row reduction:
        \begin{align*}
            \left( \begin{array}{@{}ccccc|c@{}}
                1 & 0 & 1 & -2 & 0 & 0\\
                3 & 0 & 1 & 0 & -2 & 0\\
                2 & 0 & 0 & 2 & -2 & 0\\
                2 & 0 & 0 & 1 & -2 & 0\\
                2 & 0 & 1 & -2 & -1 & 0 \\
            \end{array} \right) &\to
            \left( \begin{array}{@{}ccccc|c@{}}
                1 & 0 & 1 & -2 & 0 & 0\\
                0 & 0 & -2 & 6 & -2 & 0\\
                0 & 0 & -2 & 6 & -2 & 0\\
                0 & 0 & -2 & 5 & -2 & 0\\
                0 & 0 & -1 & 2 & -1 & 0 \\
            \end{array} \right) \to 
            \left( \begin{array}{@{}ccccc|c@{}}
                1 & 0 & 0 & 1 & 0 & 0\\
                0 & 0 & 1 & 3 & 1 & 0\\
                0 & 0 & 0 & 0 & 0 & 0\\
                0 & 0 & 0 & -1 & 0 & 0\\
                0 & 0 & 0 & -1 & 0 & 0 \\
            \end{array} \right) \\
            &\to \left( \begin{array}{@{}ccccc|c@{}}
                1 & 0 & 0 & 0 & 0 & 0\\
                0 & 0 & 1 & 0 & 1 & 0\\
                0 & 0 & 0 & 1 & 0 & 0\\
                0 & 0 & 0 & 0 & 0 & 0\\
                0 & 0 & 0 & 0 & 0 & 0 \\
            \end{array} \right)
        \end{align*}
        We get that \(x_1 = 0, x_4 = 0, x_3 + x_5 = 0\). We parametrize \(x_2 = t, x_3 = s\) and get
        \[
            x = \begin{pmatrix}
                 0 \\
                 t \\
                 s \\
                 0 \\
                 -s \\
            \end{pmatrix} =
            te_2 + s\begin{pmatrix}
                 0 \\
                 0 \\
                 1 \\
                 0 \\
                 -1 \\
            \end{pmatrix}
        \]
        Thus a basis for \(E_0(T)\) is \( \left\{ e_2, \begin{pmatrix}
             0 \\
             0 \\
             1 \\
             0 \\
             -1 \\
        \end{pmatrix}\right\} \).

        \medskip

        For \(\lambda = 1\), to solve \((A - I)x = 0\), we get the system
        \begin{align*}
            \left( \begin{array}{@{}ccccc|c@{}}
                0 & 0 & 1 & -2 & 0 & 0\\
                3 & -1 & 1 & 0 & -2 & 0\\
                2 & 0 & -1 & 2 & -2 & 0\\
                2 & 0 & 0 & 0 & -2 & 0\\
                2 & 0 & 1 & -2 & -2 & 0 \\
            \end{array} \right) &\to
            \left( \begin{array}{@{}ccccc|c@{}}
                0 & 0 & 1 & -2 & 0 & 0\\
                1 & -1 & 2 & -2 & 0 & 0\\
                2 & 0 & -1 & 2 & -2 & 0\\
                0 & 0 & 1 & -2 & 0 & 0\\
                0 & 0 & 2 & -4 & 0 & 0 \\
            \end{array} \right) \\
            &\to 
            \left( \begin{array}{@{}ccccc|c@{}}
                0 & 0 & 1 & -2 & 0 & 0\\
                1 & -1 & 0 & 2 & 0 & 0\\
                0 & 2 & 0 & -4 & -2 & 0\\
                0 & 0 & 0 & 0 & 0 & 0\\
                0 & 0 & 0 & 0 & 0 & 0 \\
            \end{array} \right) \\
            &\to 
            \left( \begin{array}{@{}ccccc|c@{}}
                0 & 0 & 1 & -2 & 0 & 0\\
                1 & 0 & 0 & 0 & -1 & 0\\
                0 & 1 & 0 & -2 & -1 & 0\\
                0 & 0 & 0 & 0 & 0 & 0\\
                0 & 0 & 0 & 0 & 0 & 0 \\
            \end{array} \right)
        \end{align*}
        We parametrize \(x_4 = t, x_5 = s\) to get
        \[
            x = \begin{pmatrix}
                s  \\
                2t + s  \\
                2t  \\
                t  \\
                s  \\
            \end{pmatrix} = 
            s \begin{pmatrix}
                 1 \\
                 1 \\
                 0 \\
                 0 \\
                 1 \\
            \end{pmatrix} +
            t \begin{pmatrix}
                 0 \\
                 2 \\
                 2 \\
                 1 \\
                 0 \\
            \end{pmatrix}
        \]
        Thus a basis for \(E_1(T)\) is \(\left\{ \begin{pmatrix}
            1 \\
            1 \\
            0 \\
            0 \\
            1 \\
       \end{pmatrix},\begin{pmatrix}
            0 \\
            2 \\
            2 \\
            1 \\
            0 \\
        \end{pmatrix} \right\} \).

        \medskip

        For \(\lambda = -1\), we solve \((A + I)x = 0\):
        \begin{align*}
            \left( \begin{array}{@{}ccccc|c@{}}
                2 & 0 & 1 & -2 & 0 & 0\\
                3 & 1 & 1 & 0 & -2 & 0\\
                2 & 0 & 1 & 2 & -2 & 0\\
                2 & 0 & 0 & 2 & -2 & 0\\
                2 & 0 & 1 & -2 & 0 & 0 \\
            \end{array} \right) &\to 
            \left( \begin{array}{@{}ccccc|c@{}}
                2 & 0 & 1 & -2 & 0 & 0\\
                1 & 1 & 0 & 2 & -2 & 0\\
                0 & 0 & 0 & 4 & -2 & 0\\
                0 & 0 & -1 & 4 & -2 & 0\\
                0 & 0 & 0 & 0 & 0 & 0 \\
            \end{array} \right) \\
            &\to \left( \begin{array}{@{}ccccc|c@{}}
                1 & 1 & 0 & 2 & -2 & 0\\
                2 & 0 & 1 & -2 & 0 & 0\\
                0 & 0 & -1 & 4 & -2 & 0\\
                0 & 0 & 0 & 4 & -2 & 0\\
                0 & 0 & 0 & 0 & 0 & 0 \\
            \end{array} \right) \\
            &\to \left( \begin{array}{@{}ccccc|c@{}}
                1 & 1 & 0 & 2 & -2 & 0\\
                0 & -2 & 1 & -4 & 4 & 0\\
                0 & 0 & -1 & 4 & -2 & 0\\
                0 & 0 & 0 & 4 & -2 & 0\\
                0 & 0 & 0 & 0 & 0 & 0 \\
            \end{array} \right) \\
            &\to \left( \begin{array}{@{}ccccc|c@{}}
                1 & 1 & 0 & 2 & -2 & 0\\
                0 & -2 & 0 & 0 & 2 & 0\\
                0 & 0 & -1 & 4 & -2 & 0\\
                0 & 0 & 0 & 4 & -2 & 0\\
                0 & 0 & 0 & 0 & 0 & 0 \\
            \end{array} \right) \\
            &\to \left( \begin{array}{@{}ccccc|c@{}}
                1 & 1 & 0 & 0 & -1 & 0\\
                0 & -2 & 0 & 0 & 2 & 0\\
                0 & 0 & -1 & 0 & 0 & 0\\
                0 & 0 & 0 & 4 & -2 & 0\\
                0 & 0 & 0 & 0 & 0 & 0 \\
            \end{array} \right) \\
            &\to \left( \begin{array}{@{}ccccc|c@{}}
                1 & 1 & 0 & 0 & -1 & 0\\
                0 & -1 & 0 & 0 & 1 & 0\\
                0 & 0 & -1 & 0 & 0 & 0\\
                0 & 0 & 0 & 1 & -\frac{1}{2} & 0\\
                0 & 0 & 0 & 0 & 0 & 0 \\
            \end{array} \right)\\
            &\to \left( \begin{array}{@{}ccccc|c@{}}
                1 & 0 & 0 & 0 & 0 & 0\\
                0 & -1 & 0 & 0 & 1 & 0\\
                0 & 0 & -1 & 0 & 0 & 0\\
                0 & 0 & 0 & 1 & -\frac{1}{2} & 0\\
                0 & 0 & 0 & 0 & 0 & 0 \\
            \end{array} \right)\\
        \end{align*}
        Let \(x_5 = t\). The general solution is
        \[
            x = t\begin{pmatrix}
                 0 \\
                 2 \\
                 0 \\
                 1 \\
                 0 \\
            \end{pmatrix}
        \]
        So a basis for \(E_{-1}(T)\) is \(\left\{ \begin{pmatrix}
            0 \\
            2 \\
            0 \\
            1 \\
            0 \\
       \end{pmatrix} \right\}\).

        \item Determine if $T$ is diagonalizable, and if so, find a basis $\beta$ so that $[T]_\beta$ is diagonal.
        
        Since the dimension of each eigenspace matches the algebraic multiplicity of each corresponding eigenvalue, \(T\) is diagonalizable and the basis \(\beta\) that makes \([T]_\beta\) diagonal is exactly the basis consisting of the basis vectors of each eigenspace. In particular,
        \[
            \beta = \left\{ e_2, \begin{pmatrix}
                0 \\
                0 \\
                1 \\
                0 \\
                -1 \\
           \end{pmatrix},\begin{pmatrix}
            1 \\
            1 \\
            0 \\
            0 \\
            1 \\
       \end{pmatrix},\begin{pmatrix}
            0 \\
            2 \\
            2 \\
            1 \\
            0 \\
        \end{pmatrix},\begin{pmatrix}
            0 \\
            2 \\
            0 \\
            1 \\
            0 \\
       \end{pmatrix} \right\}.
        \]
    \end{enumerate}
    \end{question}
    \newpage
    \begin{question}{3}
    \begin{enumerate}[label=(\alph*)]
        \item Read the proof of Theorem 58 from the additional file in the Week 10 Readings on the course page.
        \item Prove Part 1 of Theorem 59 using a strategy similar to the proof of Theorem 58. (You cannot use other parts of Theorem 59 in this proof.)
    \end{enumerate}
    Let \(A \in M_n(\mathbb{F})\) with \(n \geq 2\). If \(A\) has a row of 0's, then \(\det A = 0\).
    \tcblower
    Write \(A = \begin{pmatrix}
         r_1 \\
         \vdots \\
         r_n \\
    \end{pmatrix}\), where \(r_j\) represents the rows of \(A\). Suppose that \(r_i = \vec{0}\). If \(i = 1\), the result is immediate by cofactor expansion. Otherwise, if \(i > 1\), we do induction on \(n\).

    Let \(n = 2\). The only possibility is \(i = 2\), so denote
    \[
        A = \begin{pmatrix}
            a &  b \\
            0 &  0 \\
        \end{pmatrix}.
    \]
    From here, it is easy to see that \(\det A = 0\).

    Now, suppose that this is true for some \(n\). We will show that it is true for \(n + 1\). Define \(\tilde{r}_{j,k}\) to be the row obtained by deleting the \(k\)th entry of \(r_j\). Using cofactor expansion along the first row, we have
    \[
        \det A = \sum_{k=1}^{n+1} A_{1k}\det \tilde{A}_{1k}
    \]
    Observe that \(\tilde{A}_{1k}\) are \(n\times n\) matrices, and since the \(i\)th row was 0 in the original matrix (and \(i > 1\)), the \(i - 1\)th row in \(\tilde{A}_{1k}\) is 0, so by the induction hypothesis \(\det \tilde{A}_{1k} = 0\), thus \(\det A = 0\) and we are done.

    \end{question}
    \newpage
    \begin{question}{4}
    Assume that Parts 1 and 2 of Theorem 59 have been proved. You cannot use Parts 4 through 7 of Theorem 59 in the following problem.
    
    \begin{enumerate}[label=(\alph*)]
        \item Prove Part 3 using induction on $n$. (Check $n = 1, 2$ by hand, then in the inductive step assume $n+1 \ge 3$.)
        \item Prove Part 4 using row-swapping matrices and properties of determinants.
    \end{enumerate}
    \tcblower
    \ 

    (a):

    We prove Part 3 using induction on \(n\).

    Let \(n = 1\). The statement is vacuously true, as \(A\) cannot have 2 identical rows.

    Let \(n = 2\). Then it must be true that
    \[
        A = \begin{pmatrix}
            a &  b \\
            a &  b \\
        \end{pmatrix}, \text{ for } a,b \in \mathbb{F}.
    \]
    Then \(\det A = ab - ab = 0\) as expected.

    Now, suppose that this statement is true for some \(n \in \mathbb{N}\), where \(n > 1\). We will show it also holds for \(n + 1\). Let \(r_i,r_j\) be the identical rows. Since \(n + 1 > 2\), we are guaranteed to have one other row \(r_k\) that is not \(r_i\) or \(r_j\). We perform a row \(k\) expansion of \(\det A\) and see that
    \[
        \det A = \sum_{l=1}^{n + 1} A_{kl} \det \tilde{A}_{kl}
    \]
    Notice that \(\tilde{A}_{kl}\) is a \(n\times n\) matrix, and contain both \(r_i\) and \(r_j\) with the \(l\)th entry deleted. But these rows are still identical because the same entry got deleted. By the induction hypothesis,
    \[
        \det A = \sum_{l=1}^{n + 1} A_{kl} 0 = 0
    \]
    which was what we wanted.

    \bigskip

    (b):

    Suppose that \(B\) is obtained from \(A\) by swapping row \(i\) and row \(j\). Denote these rows as \(r_i,r_j\) respectively. Using linearity in one row of the determinant, and the previous result we proved,
    \[
        0 = \det \begin{pmatrix}
             r_1 \\
             \vdots \\
             r_i + r_j\\
             \vdots \\
             r_i + r_j \\
             \vdots \\
             r_n
        \end{pmatrix} =
        \det \begin{pmatrix}
            r_1 \\
            \vdots \\
            r_i\\
            \vdots \\
            r_i + r_j \\
            \vdots \\
            r_n
       \end{pmatrix} +
       \det \begin{pmatrix}
            r_1 \\
            \vdots \\
            r_j\\
            \vdots \\
            r_i + r_j \\
            \vdots \\
            r_n
        \end{pmatrix}
    \]
    \[
        \implies 0 = \det \begin{pmatrix}
            r_1 \\
            \vdots \\
            r_i\\
            \vdots \\
            r_j \\
            \vdots \\
            r_n
       \end{pmatrix}+
       \det \begin{pmatrix}
            r_1 \\
            \vdots \\
            r_i\\
            \vdots \\
            r_i \\
            \vdots \\
            r_n
        \end{pmatrix}+
        \det \begin{pmatrix}
            r_1 \\
            \vdots \\
            r_j\\
            \vdots \\
            r_i \\
            \vdots \\
            r_n
       \end{pmatrix}+
       \det \begin{pmatrix}
            r_1 \\
            \vdots \\
            r_j\\
            \vdots \\
            r_j \\
            \vdots \\
            r_n
        \end{pmatrix}
    \]
    \[
        \implies 0 = \det A + 0 + \det B + 0
    \]
    \[
        \implies \det B = -\det A
    \]
    as needed.
    \end{question}
    \newpage
    \begin{question}{5}
    Prove that if $U \in M_{n \times n}(F)$ is upper triangular, then $\det U = \prod_{i=1}^n U_{ii}$.
    \tcblower
    We proceed using induction on \(n\). If \(n=1\), the result is immediate. Suppose that the statement holds for some \(n \in \mathbb{N}\). We will show the same is the case for \(n + 1\).

    Let \(U \in M_{n+1} (\mathbb{F})\) be upper triangular. We have
    \[
        \det U = \sum_{j=1}^{n+1} U_{1j} \det \tilde{U}_{1j}.
    \]
    For \(j \neq 1\), notice that the entries of the first column of \(\tilde{U}_{1j}\) are 0, as \((\tilde{U}_{1j})_{i1} = U_{(i+1)1} = 0\). Thus
    \[
        \det \tilde{U}_{1j} = \det \tilde{U}_{1j}^t = 0
    \]
    as the transpose has a row of 0's. The cofacter expansion of \(\det U\) reduces to
    \[
        \det U = U_{11} \det \tilde{U}_{11}
    \]
    but \(\tilde{U}_{11} \in M_n(\mathbb{F})\) is upper triangular, so
    \[
        \det U = U_{11} \prod _{i=1}^n \tilde{U}_{ii} = U_{11} \prod _{i=2}^{n+1} U_{ii} = \prod _{i=1}^{n+1} U_{ii}. 
    \]
    which completes the proof.
    \end{question}
    \newpage
    \begin{question}{6}
    Let $V$ be a vector space over $F$, and $T : V \to V$ a linear map. If $W \subseteq V$ is a $T$-invariant subspace, then we can restrict \(T\) to \(W\), to obtain a map \(T_W : W \to W\). We call \(T_W\) the restriction map.
    
    \begin{enumerate}[label=(\alph*)]
        \item Let \(\beta _W\) be a basis for \(W\). In HW\#3 we proved that if \(\beta = \beta _W \beta _1\) is an extension of \(\beta_W\) to a basis for \(V\), then \([T]_\beta = \left( \begin{array}{@{}c|c@{}}
            A & B \\
            \hline
            O & C \\
        \end{array}\right)\). Prove that \(A = [T_W]_{\beta_W}\).

        \item Let \(M = \left(\begin{array}{@{}c|c@{}}
            A & B \\
            \hline
            O & C \\
        \end{array}\right)\). Prove that \(\det M = \det A \det C\).
    \end{enumerate}
    \tcblower
    \ 

    (a):

    Let \(n = \dim V\), \(k = \dim W\). Denote \(\beta = \{ w_1, ..., w_n \}\). Then the \(j\)th column of \(\left( \begin{array}{@{}c@{}}
        A \\
        \hline
        O \\
    \end{array} \right)\) is \([T(w_j)]_\beta\), so
    \[
        T(w_j) = \sum_{i=1}^{k} A_{ij} w_i.
    \]
    But since \(w_j \in W\), we have
    \[
        T_W(w_j) = T(w_j) = \sum_{i=1}^{k} A_{ij} w_i.
    \]
    which implies that \([T_W(w_j)]_{\beta_W}\) is exactly the \(j\)th column of \(A\), from which we can conclude that \([T_W]_{\beta_W} = A\).

    \medskip

    (b):

    We will use the following 2 lemmas:

    \textbf{Lemma 1:} Let \(k < n\). For matrices \(B \in M_{k\times (n-k)}(\mathbb{F})\), \(C \in M_{(n-k)\times (n-k)}(\mathbb{F})\), Let \(M = \left( \begin{array}{@{}c|c@{}}
        I_k & B \\
        \hline
        O & C
    \end{array} \right) \in M_n(\mathbb{F})\). Then \(\det M = \det C\).

    Proceed using induction on \(k\). If \(k = 1\), then
    \[
        \det M = \det C + \sum_{j=2}^{n} M_{1j} \det \tilde{M}_{1j}.
    \]
    For \(j > 1\), \(\tilde{M}_{1j}\) has a column full of 0's, so \(\det \tilde{M}_{1j} = 0\) and the result follows.

    Now suppose that the lemma is true for some \(k \in \mathbb{N}\). Using a similar argument,
    \[
        \det M = \sum_{j=1}^{n} M_{1j} \det \tilde{M}_{1j} = \det \tilde{M}_{11} + \sum_{j=k+2}^{n} M_{1j} \det \tilde{M}_{1j} = \det \tilde{M}_{11}
    \]
    Notice that \(\tilde{M}_{11}\) satisfies our assumption in the induction hypthoesis, so \(\det M = \det \tilde{M}_{11} = \det C\).

    \medskip

    \textbf{Lemma 2:} \(\det \left( \begin{array}{@{}c|c@{}}
        A & O \\
        \hline
        O & I
    \end{array} \right) = \det A\).

    The proof is analogous to the proof of Lemma 1, so we omit it. We now proceed with the main result.

    First, consider the case where \(A\) is not invertible. Then its columns are linearly dependent. But this means that \(M\) also has linearly dependent columns, so \(\det M = 0 = \det A \det C\).

    Otherwise, if \(A\) is invertible, define \(N = \left( \begin{array}{@{}c|c@{}}
        A^{-1} & O \\
        \hline
        O & I
    \end{array} \right)\). Then \(MN = \left( \begin{array}{@{}c|c@{}}
            I & B \\
            \hline
            O & C
        \end{array} \right)\) and we get
    \[
        \det C = \det MN = \det M \det N = \det M \det A^{-1} = \frac{\det M}{\det A}
    \]
    \[
        \implies \det M = \det A \det C
    \]
    and we are done.
    \end{question}
    \newpage
    \begin{question}{7}
    Deduce from Question 6 that if $W$ is a $T$-invariant subspace, then $C_{T_W}$ divides $C_T$.
    \tcblower
    Suppose that \(W\) is a \(T\)-invariant subspace. Fix a basis \(\beta\) such that \([T]_\beta = \left( \begin{array}{@{}c|c@{}}
        [T_W]_{\beta_W} & B \\
        \hline
        O & C \\
    \end{array}\right)\). Then
    \[
        C_T(\lambda) = \det (\lambda I - T) = \det \left( \begin{array}{@{}c|c@{}}
            \lambda I_k - [T_W]_{\beta_W} & -B \\
            \hline
            O & \lambda I_{n-k} - C \\
        \end{array}\right) = \det (\lambda I_k - T_W)\det (I_{n-k} - C)
    \]
    \[
        C_T(\lambda) = C_{T_W}(\lambda)\det (I_{n-k} - C)
    \]
    as expected.
    \end{question}
    \newpage
    \begin{question}{8}
    Let $V$ be a finite-dimensional vector space over a field $F$, and $W_1, W_2 \subseteq V$ subspaces so that $V = W_1 \oplus W_2$. Define the projection maps $P_i : V \to V$ by $P_i(x) = x_i$ where $x = x_1 + x_2$ with $x_1 \in W_1$ and $x_2 \in W_2$.
    
    \begin{enumerate}[label=(\alph*)]
        \item Prove that $P_i$ is linear.
        \item Prove that $P_i^2 = P_i$.
        \item Prove that each $W_j$ is $P_i$-invariant.
        \item Determine if $P_i$ is diagonalizable and justify your answer.
    \end{enumerate}
    \tcblower
    For convenience, we will prove the statements for \(P_1\), as the argument for \(P_2\) will be the exact same.

    (a):

    Let \(x,y \in V\), \(c \in \mathbb{F}\). Write \(x = x_1 + x_2, y = y_1 + y_2\), where \(x_i,y_i \in W_i\). Then
    \[
        P_1(cx + y) = P_1(cx_1 + y_1 + cx_2 + y_2) = cx_1 + y_1 = cP_1(x) +P_i(y)
    \]

    \bigskip

    (b):

    Let \(x = x_1 + x_2 \in V\). Then \(P_1(x) = x_1\). Notice that \(x_1 = x_1 + 0\), so \(P_1^2 x = P_1(x_1) = x_1 = P_1(x)\).

    \bigskip

    (c):

    As we have shown above, for \(x_1 \in W_1\), \(P_1(x_1) = x_1 \in W_1\), so \(W_1\) is \(P_1\)-invariant. For \(x_2 \in W_2\), we have \(P_1(x_2) = 0 \in W_2\), so \(W_2\) is also \(P_1\)-invariant.

    \bigskip

    (d):

    Let \(n_1\) be the dimensions of \(W_1\). Choose \(\beta = \beta _1 \cup \beta _2\) to be a basis for \(V\), where \(\beta _1, \beta _2\) are bases for \(W_1, W_2\) respectively. Based on part (c), we have
    \[
        [P_1]_\beta = \left( \begin{array}{@{}c|c@{}}
            I_{n_1} & O \\
            \hline
            O & O
        \end{array} \right) 
    \]
    which is a diagonal matrix, so \(P_1\) is diagonalizable.
    \end{question}
    \newpage
    \begin{question}{9}
    In this problem, we carefully define the direct sum for more than two subspaces.
    
    Let $W_1, \ldots, W_k \subseteq V$ be subspaces. We say $V = W_1 \oplus \cdots \oplus W_k$ if:
    \begin{itemize}
        \item \(V = W_1 + \cdots + W_k\)
        \item For each \(i \in \{ 1, \cdots, k \}\), we have \(W_i \cap \left( \sum_{j\neq i} W_j \right) = \{ 0 \}\).
    \end{itemize}
    
    \begin{enumerate}[label=(\alph*)]
        \item Let \(V\) be an \(n\)-dimensional vector space. Prove that every basis \(\beta\) for \(V\) gives a direct sum decomposition \(V = W_1 \oplus \cdots \oplus W_n\) where \(\dim W_i = 1\).
        \item Prove the converse of (a): If $V = W_1 \oplus \cdots \oplus W_n$ with $\dim W_i = 1$, then choosing non-zero $w_i \in W_i$ forms a basis \(\beta = \{ w_1, \ldots, w_n \}\) for $V$.
        \item Let $T : V \to V$ be linear, and \(V = W_1 \oplus \cdots \oplus W_k\), where each \(W_i\) is \(T\)-invariant. Let \(\beta _i\) be a basis for \(W_i\), and set \(\beta = \beta _1 \cup \cdots \cup \beta _k\). Show that $[T]_\beta = \left( \begin{array}{@{}c|c|c|c@{}}
            A_1 & O & \cdots & O \\
            \hline
            O & A_2 & O & O \\
            \hline
            \vdots & \vdots & \ddots & \vdots \\
            \hline
            O & \cdots & O & A_k \\
        \end{array} \right) $ is block diagonal.
    \end{enumerate}
    \tcblower
    \ 

    (a):

    For each basis element \(w_i\), set \(W_i = \mathrm{span} (w_i)\). It is immediate that \(V = W_1 + \cdots + W_n\). Only the second condition remains to be shown. Let \(i \in \{ 1, ..., n \}\). Let \(v \in W_i \cap \left( \sum_{j\neq i} W_j \right)\). This means that for some \(c_i \in \mathbb{F}\), \(-c_i w_i = v = \sum_{j\neq i} c_j w_j\). We rearrange to get that \(\sum_{j=1}^{n} c_i w_i = 0\). By linear independence of \(\beta\), \(c_i = 0\), so \(v = 0\). Thus \(V = W_1 \oplus \cdots \oplus W_n\).

    \bigskip

    (b):

    Suppose that \(V = W_1 \oplus \cdots \oplus W_n\). From each \(W_i\) pick a \(w_i \neq 0\). Since \(\dim W_i = 1\), \(\{ w_i \}\) is actually a basis for \(W_i\). Now, we show that \(\beta = \{ w_1, \ldots, w_{n} \}\) forms a basis for \(V\). Let \(v \in V\). Then \(v = v_1 + \cdots + v_n\), where \(v_i \in W_i\). But each \(v_i\) can be written as \(c_i w_i\), for some \(c_i \in \mathbb{F}\), so
    \[
        v = \sum_{i=1}^{n} c_i w_i
    \]
    Next, let \(\sum_{i=1}^{n} c_i w_i = 0\). For each \(j \in \{ 1, ..., n \}\) We have that \(-c_{j}  w_j = \sum_{i\neq j} c_i w_i\). This means that \(-c_j w_j\) is an element of both \(W_j\) and \(\left( \sum_{i\neq j} W_i \right)\), so \(-c_j w_j = 0\), meaning \(c_j = 0\) for each \(j\). Thus we can conclude that \(\beta\) is a basis for \(V\).

    \bigskip

    (c):

    Proceed by using induction on \(k\). If \(k = 1\), the entire matrix itself is the block, so the result is trivial.

    Suppose the statement holds for some \(k\). We want to show it for \(k+1\). Let \(V = W_1 \oplus \cdot \oplus W_k \oplus W_{k+1}\). Since each \(W_i\) is \(T\)-invariant, it follows that \(W' \coloneqq W_1 \oplus \cdots \oplus W_k\) is \(T\)-invariant. Thus
    \[
        [T]_\beta = \left( \begin{array}{@{}c|c@{}}
            A & O \\
            \hline
            O & A_{k+1} \\
        \end{array} \right)
    \]
    where \(A = [T_{W'}]_{\beta'}\), \(A_{k+1} = [T_{W_{k+1}}]_{\beta_{k+1}}\), and \(\beta ' = \beta _1 \cup \cdots \cup \beta _k\). Note that the top right quadrant is \(O\) because \(W_{k+1}\) is \(T\)-invariant. Finally, by our induction hypothesis, \(A\) is actually block diagonal, so
    \[
        [T]_\beta = \left( \begin{array}{@{}c|c@{}}
            A & O \\
            \hline
            O & A_{k+1} \\
        \end{array} \right) = \left( \begin{array}{@{}c|c|c|c@{}}
            A_1 & O & \cdots & O \\
            \hline
            O & A_2 & O & O \\
            \hline
            \vdots & \vdots & \ddots & \vdots \\
            \hline
            O & \cdots & O & A_k \\
        \end{array} \right)
    \]
    and we are done.
    \end{question}
    \newpage
    \begin{question}{10}
    Let $W_1, \ldots, W_k \subseteq V$ be subspaces of \(V\) with bases $\beta_1, \ldots, \beta_k$. Prove that $V = W_1 \oplus \cdots \oplus W_k$ if and only if $\beta = \beta_1 \cup \cdots \cup \beta_k$ is a basis for $V$.
    \tcblower
    We proceed with induction on \(k\). For \(k = 1\), the result is obvious. Now suppose it holds for some \(k\). Then using a result from the first homework, \(V = W_1 \oplus \cdots \oplus W_{k+1}\) if and only if \(\beta ' \cup \beta_{k+1}\) is a basis for \(V\), where \(\beta ' = \beta _1 \cup \cdots \cup \beta _k\) is a basis for \(W_1 \oplus \cdots \oplus W_k\), which we know from the induction hypothesis. Thus \(\beta = \beta _1 \cup \cdots \cup \beta _{k+1}\), so the equivalence in statements has been shown.
    \end{question}
    \newpage
    \begin{question}{11}
    Determine whether the following statements are true or false. Justify your answers.
    
    \begin{enumerate}[label=(\alph*)]
        \item If $V = W_1 \oplus W_2$ and $T_{W_1}, T_{W_2}$ are diagonalizable, then $T$ is diagonalizable.
        \item If $W_i \cap W_j = \{0\}$ for $i \neq j$ and $V = W_1 + W_2 + W_3$, then $V = W_1 \oplus W_2 \oplus W_3$.
        \item Let \(V\) be a finite dimensional vector space over \(\mathbb{F}\) and \(T: V \to V\) be a linear map. If $\dim V = 7$, $\dim N(T) = 3$, and $\operatorname{rank}(T - I) = 4$, then $T$ is diagonalizable.
    \end{enumerate}
    \tcblower
    \ 

    (a):

    This statement is true. Suppose that \(V = W_1 \oplus W_2\) and \(T_{W_1}, T_{W_2}\) are diagonalizable. Pick bases \(\beta _1, \beta _2\) for \(W_1, W_2\) such that \(A = [T_{W_1}], B = [T_{W_2}]\) are diagonal. It follows that \(\beta = \beta _1 \cup \beta _2\) is a basis for \(V\) and moreover
    \[
        [T]_\beta = \left( \begin{array}{@{}c|c@{}}
            A & O \\
            \hline
            O & B
        \end{array} \right)
    \]
    which is diagonal.

    \bigskip

    (b):

    This statement is true. Let \(\beta _1, \beta _2, \beta _3\) be bases for \(W_1, W_2, W_3\). Since \(W_1 \cap W_2 = \{ 0 \}\), then \(W' = W_1 + W_2\) is a direct sum of the subspaces \(W_1\) and \(W_2\) and a basis for \(W'\) is given by \(\beta ' = \beta _1 \cup \beta _2\). As well, from our assumption, \(\beta _1, \beta _2, \beta _3\) are pairwise disjoint so \(\beta_3\) is disjoint from \(\beta '\). It follows that \(\beta = \beta _1 \cup \beta _2 \cup \beta _3\) is a basis for \(V = W_1 + W_2 + W_3\), so we indeed have \(V = W_1 \oplus W_2 \oplus W_3\).

    \bigskip

    (c):

    This statement is false. Let \(V = \mathbb{F}^7\) with standard basis \(\beta\). Take \(T = T_A\), where
    \[
        A = \begin{pmatrix}
            0 & 0 & 0 & 0 & 0 & 0 &  0 \\
            0 & 0 & 0 & 0 & 0 & 0 &  0 \\
            0 & 0 & 0 & 1 & 0 & 0 &  0 \\
            0 & 0 & 0 & 0 & 0 & 0 &  0 \\
            0 & 0 & 0 & 0 & 1 & 0 &  0 \\
            0 & 0 & 0 & 0 & 0 & 1 &  0 \\
            0 & 0 & 0 & 0 & 0 & 0 &  1 \\
        \end{pmatrix}
    \]
    Clearly \(\rank T = 4\), so \(\mathrm{nullity} (T) = 3\). As well,
    \[
        [T - I]_\beta = \begin{pmatrix}
            -1 & 0 & 0 & 0 & 0 & 0 &  0 \\
            0 & -1 & 0 & 0 & 0 & 0 &  0 \\
            0 & 0 & -1 & 1 & 0 & 0 &  0 \\
            0 & 0 & 0 & -1 & 0 & 0 &  0 \\
            0 & 0 & 0 & 0 & 0 & 0 &  0 \\
            0 & 0 & 0 & 0 & 0 & 0 &  0 \\
            0 & 0 & 0 & 0 & 0 & 0 &  0 \\
        \end{pmatrix}
    \]
    has rank 4. But observe that \(C_T(\lambda) = \lambda ^4 (\lambda - 1)^3\) so the eigenvalue \(0\) has multiplicity 4, but the eigenspace \(E_0(T) = N(A)\) has dimension \(3\), so \(T\) is not diagonalizable.
    \end{question}
\end{document}